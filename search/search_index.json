{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Trixie HPC wiki","text":""},{"location":"#trixie-status","title":"Trixie Status","text":""},{"location":"#trixie-not-just-an-gpu-cluster","title":"Trixie - not just an GPU Cluster","text":"<p>The name for this AI GPU cluster comes from a very inspirational women Beatrice \"Trixie\" Helen Worsley who helped shape the state of computer science in Canada and abroad. She even worked briefly for the National Research Council of Canada in the late forties.</p>"},{"location":"#trixie-availability","title":"Trixie availability","text":"<p>Trixie is designated for collaborative research in support of the NRC AI4D Challenge Program.  It is also available for the NRC Pandemic Response (COVID) Challenge Program and some other internal NRC research initiatives on a prioritized basis.  Access is managed via projects where project participants who need access to Trixie (internal and external to NRC) are specified as part of the Trixie access request.  NRC participants must be employees of NRC or NRC volunteer visitors with current agreements at NRC.  External users must be a principle investigator (PI) named in a grant, contribution and/or collaborative agreement with NRC, or be a student, post doc, or research associate directly supervised by that PI.</p> <p>Trixie access requests can be completed by the NRC PI in consultation with their external collaborators.  Access will only be granted for approved NRC (internal or collaborative) projects.</p> <p>The access request form is available here. Once completed, the form can be submitted to the Trixie program manager, Mark Stoochnoff.</p>"},{"location":"#nrc-authorized-users-only-utilisateurs-autorises-du-cnrc-seulement","title":"NRC authorized users only - Utilisateurs autoris\u00e9s du CNRC seulement","text":"<p>Access to the National Research Council of Canada Information Technology (IT) systems and resources by employees and any other person must be authorized. All users shall comply with the NRC Policy on Acceptable Network and Device Use (PANDU). All activities done using these systems and resources are subject to monitoring.</p> <p>NOTICE: Anyone using these systems and resources by their access consents to such monitoring, and has read and understands the responsibilities outlined within the PANDU. Unauthorized use and PANDU violations may result in disciplinary action and/or criminal prosecution.</p> <p>Les employ\u00e9s du Conseil national de recherches du Canada (CNRC) et autres utilisateurs qui acc\u00e8dent aux syst\u00e8mes et aux ressources infotechnologiques du CNRC doivent \u00eatre autoris\u00e9s \u00e0 le faire. Ils doivent en outre respecter la Politique sur l\u2019utilisation acceptable des dispositifs et des r\u00e9seaux (PUADR) du CNRC. Toute utilisation des syst\u00e8mes et ressources peut faire l\u2019objet d\u2019une surveillance. AVIS : Quiconque utilise les syst\u00e8mes et ressources infotechnologiques du CNRC consent par le fait m\u00eame \u00e0 faire l\u2019objet d\u2019une surveillance et atteste avoir pris connaissance de ses responsabilit\u00e9s en vertu de la PUADR. Toute utilisation non autoris\u00e9e du mat\u00e9riel ou tout manquement \u00e0 la PUADR pourrait entra\u00eener des mesures disciplinaires, voire une poursuite au criminel.</p>"},{"location":"#policy-on-acceptable-network-and-device-use-pandu","title":"Policy on acceptable network and device use (PANDU)","text":"<p>All users shall comply with the NRC Policy on Acceptable Network and Device Use (PANDU)</p> <p>PANDU</p>"},{"location":"#politique-sur-lutilisation-acceptable-des-dispositifs-et-des-reseaux-puadr","title":"Politique sur l'utilisation acceptable des dispositifs et des r\u00e9seaux (PUADR)","text":"<p>Tous les utilisateurs doivent respecter la Politique sur l\u2019utilisation acceptable des dispositifs et des r\u00e9seaux (PUADR)</p> <p>PUADR</p>"},{"location":"#getting-an-account","title":"Getting an account","text":"<p>Accounts will be generated as required based on approved Trixie access request forms.  If you have an approved access request, but have not received account information, you can contact the AI4D Program Manager, Patricia Oakley for assistance.</p>"},{"location":"#username-and-password","title":"Username and Password","text":"<p>Your username and password are the same as your RES (orange) account.</p>"},{"location":"#connecting-to-the-machine","title":"Connecting to the machine","text":"<p>The connection path will vary depending from which network you are trying to access.</p> <ul> <li>From Legacy (black) and RES (orange) (wired or over 2 Factor VPN) users can connect directly via SSH using the host name: trixie.res.nrc.gc.ca</li> <li>Non-NRC collaborators can access Trixie via a bastion host. Please speak to your NRC contact to get access and additional details. Once you receive your access credentials please see External Access Setup to configure your local computer to access Trixie.</li> </ul>"},{"location":"#file-transfers","title":"File Transfers","text":""},{"location":"#running-jobs","title":"Running jobs","text":"<p>Jobs on Trixie must be run via the SLURM job scheduler. Do NOT run jobs on the headnode. Users who run on the head node risk account suspension. Running jobs</p>"},{"location":"#containers","title":"Containers","text":"<p>Containers can be run on the cluster either as jobs (via singularity/podman) or as services within the cluster (via podman/rootless-docker). See Containers for more details.</p>"},{"location":"#description-of-the-cluster","title":"Description of the cluster","text":"<p>Hardware</p>"},{"location":"#available-software","title":"Available software","text":"<p>Available software</p>"},{"location":"#pytorchdistributed","title":"pytorch.distributed","text":"<p>SLURM, pytorch distributed and Multiple Nodes</p>"},{"location":"Account-Codes/","title":"Account Codes","text":"<p>In order to run jobs on Trixie, users need to specify which SLURM Account Code should be used for tracking purposes and possibly billing. This is handled by adding a line in the SLURM submission script which identifies the account.</p> <pre><code>SBATCH --account=account_code\n</code></pre> <p>IMPORTANT NOTE: Users must be authorized to charge an account before they can use it.</p>"},{"location":"Account-Codes/#projects","title":"Projects","text":"<p>Projects are typically designated for a fixed period of time and are associated with projects defined in SAP. They allow project members access to Trixie and usage can be tracked for each project.</p> Projects aero-apdc-01 aero-dtvt-01 aero-inbr6-01 aero-smpl-070-1 ai4d-bio-02 ai4d-bio-04a ai4d-bio-145 ai4d-bio-146 ai4d-core-01 ai4d-core-06 ai4d-core-08 ai4d-core-09 ai4d-core-132 ai4d-core-147 ai4d-core-148 ai4d-core-150 ai4d-core-uk1 ai4d-mat-03 ai4d-mat-04 ai4d-mat-09 ai4d-photo-01a ai4d-photo-01c ai4d-photo-08 ai4d-photo-135 ai4d-photo-164 ai4l-taw-110 ai4l-taw-133 aip-adl-05 aip-iot-032 aip-jpn-302 dt-asec-01 dt-caisi-01 dt-cbmi-01 dt-inbr6-1156 sdt-aqc-007 sdt-aqc-208 sdt-qsp-051"},{"location":"Account-Codes/#teams","title":"Teams","text":"<p>Teams are typically associated with a team or a particular group within a NRC Research Centre. They allow team members to access Trixie for non project based work.</p> Research Centre Team Code Team Name Aquatic and Crop Resource Development acrd-base Default base Clean Energy ce-amdpo Acceleration of Materials Discovery and Process Optimization Digital Technologies dt-cdro Chief Digital Research Office dt-cvg Computer Vision and Graphics dt-cyber Cybersecurity dt-dac Data Analytics Centre dt-dscs Data Science for Complex Systems dt-hci Human-Computer Interaction dt-mtp Multilingual Text Processing dt-student Team for student access to Trixie dt-ta Text Analytics Security and Disruptive Technologies sdt-clean Computational Laboratory for Energy And Nanoscience"},{"location":"Automatically-Resuming-Requeueing/","title":"How to I get my job to requeue after my time limit?","text":"<p>Here\u2019s a skeleton of what our jobs look like.  Please check your job once it is running to dial down the number of cpus and memory needed.  If we don\u2019t use the node\u2019s full resources, it would be nice to be able to submit other cpu-only jobs, aka none gpu jobs on those nodes.</p> <p>Important steps in order to get automatic requeueing working: * Ask slurm to send you a signal 30 seconds before the end of your time limit <code>--signal=B:USR1@30</code> * Have a thread listen to the requested signal <code>trap _requeue USR1</code> * Send your MAIN process in the background and wait for it otherwise your <code>_requeue</code> function will NEVER get a chance to run.</p> <pre><code>#SBATCH --job-name=WMT21.training\n#SBATCH --comment=\"Thanks Samuel Larkin for showing me how to work with Slurm\"\n\n#SBATCH --partition=TrixieMain\n#SBATCH --account=dt-mtp\n#SBATCH --gres=gpu:4\n#SBATCH --time=12:00:00                                                                                                                                                      #SBATCH --exclude=cn125\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1                                                                                                                                                  #SBATCH --cpus-per-task=24\n#SBATCH --mem=40G\n# To reserve a whole node for yourself\n####SBATCH --exclusive\n#SBATCH --open-mode=append\n#SBATCH --requeue\n#SBATCH --signal=B:USR1@30\n#SBATCH --output=%x-%j.out                                                                                                                                                   \n\n# Requeueing on Trixie\n# [source](https://www.sherlock.stanford.edu/docs/user-guide/running-jobs/)\n# [source](https://hpc-uit.readthedocs.io/en/latest/jobs/examples.html#how-to-recover-files-before-a-job-times-out)\nfunction _requeue {\n   echo \"BASH - trapping signal 10 - requeueing $SLURM_JOBID\"\n   date\n   # This would allow to generically requeue any job but since we are using XLM\n   # which is slurm aware, XLM could save its model before requeueing.\n   scontrol requeue $SLURM_JOBID\n}\n\nif [[ -n \"$SLURM_JOBID\" ]]; then\n   trap _requeue USR1\nfi\n\n\ntime python \u2013m sockeye.train \u2026. &amp;\nwait\n</code></pre> <p>where <code>time python \u2013m sockeye.train \u2026.</code> is the process you want to run.</p>"},{"location":"Available-Software/","title":"Available Software","text":"<p>The most up-to-date way to see which software has been preinstalled on Trixie is by using the module command. When in double, it is the definitive list.</p> <p>Software on Trixie is organized using the <code>module</code> service. Users can load, unload, and swap libraries within their environment and job submission scripts via <code>modules</code>.</p> <p>If there is a piece of software you would like to use but it is not available from the list (and you can't figure out how to build it yourself in your home directory), you may create a request using the issues tab (https://github.com/ai4d-iasc/trixie/issues) and please tag a member of Research Platform Support team (e.g., rloveNRC, nrcfieldsa, NRCGavin) to push a notification to the team. Please do not create duplicate requests for software (but feel free to comment on a thread to ''upvote'' the priority list is clear.</p>"},{"location":"Available-Software/#compilers","title":"Compilers","text":"<ul> <li>gcc</li> <li>intel (not currently available, in procurement)</li> </ul>"},{"location":"Available-Software/#numerical-libraries","title":"Numerical libraries","text":"<ul> <li>intel-mkl (not currently available, in procurement)</li> </ul>"},{"location":"Available-Software/#deep-learning-frameworks","title":"Deep learning frameworks","text":"<ul> <li>tensorflow</li> <li>pytorch</li> </ul>"},{"location":"Available-Software/#python","title":"Python","text":"<ul> <li>system python</li> <li>CC stack python (now default):</li> <li>virtualenvs</li> <li>Anaconda, miniconda:</li> <li>System-wide Anaconda</li> <li>User miniconda</li> </ul>"},{"location":"Available-Software/#scientific-simulation-software","title":"Scientific simulation software","text":"<ul> <li>abinit</li> <li>lumerical</li> </ul>"},{"location":"Contribute/","title":"Contribute","text":"<p>You have a better way of doing something on the cluster? You know how to do something that isn't documented in this Wiki? You would like to share your knowledge with others? We would appreciate that you share your knowledge by providing a PR with your new documentation. Please open a Pull Request.</p> <p>Thanks for your contribution!</p>"},{"location":"External-Access-Advanced-Configuration/","title":"External Access Advanced Configuration","text":""},{"location":"External-Access-Advanced-Configuration/#overview","title":"Overview","text":"<p>To ease usage of the bastion host to easily connect to Trixie, there are some steps which can be taken, especially making use of the SSH ProxyJump and ControlMaster parameters. Basically, you need to configure SSH to automatically connect with the Trixie server using the bastion host as a connector between your local computer and the Trixie server.</p> <p>Important Note: Before proceeding with this configuration, please ensure that you have performed the External Access Setup procedure.</p>"},{"location":"External-Access-Advanced-Configuration/#mac-osx-linux","title":"Mac OSX / Linux","text":"<p>To configure SSH to automatically connect to the Trixie server, please perform the following steps</p> <ol> <li>Open your <code>.ssh/config</code> file with your preferred text editor and add the following lines on    your local machine \u2013 not the servers \u2013 while substituting your given usernames in the User    directive. TIP: Some users have reported that using <code>doej</code> did not work for their PUB username.    Please try using the first.last format - for example <code>john.doe</code> - for your PUB username if    <code>doej</code> doesn't work for you</li> </ol> <pre><code>Host trixie-bastion\n  HostName trixie.nrc-cnrc.gc.ca\n  User &lt;firstname&gt;.&lt;lastname&gt;@pub.nrc-cnrc.gc.ca\n  ForwardAgent true\n\nHost trixie\n  HostName trixie.res.nrc.gc.ca \n  User ext.&lt;firstname&gt;.&lt;lastname&gt;\n  ProxyJump trixie-bastion\n</code></pre> <p>Once your settings are configured, you will be able to login to the Trixie server with the following command</p> <p><code>ssh trixie</code></p> <p>Please note that you will be prompted as follows</p> <ol> <li>Prompt for your PUB password</li> <li>LoginTC prompt \u2013 enter 1</li> <li>Prompt for your RES ext password</li> </ol>"},{"location":"External-Access-Advanced-Configuration/#windows-putty","title":"Windows \u2013 Putty","text":"<p>To configure SSH to automatically connect to the Trixie server, please set the following settings in your Putty application, substituting your username where applicable.</p> <ol> <li>Under Connection -&gt; Proxy<ol> <li>From the Proxy type dropdown, select: SSH to proxy and use port forwarding</li> <li>Set Proxy hostname: trixie.nrc-cnrc.gc.ca</li> <li>Set Port: 22</li> <li>Set Username: &lt;firstname.lastname&gt;@pub.nrc-cnrc.gc.ca</li> <li>Leave Password blank - do not fill it in </li> </ol> </li> <li>Under Connection -&gt; SSH -&gt; X11<ol> <li>Select the option Enable X11 forwarding </li> </ol> </li> <li>Under Session<ol> <li>Set Host Name (or IP address): ext.&lt;firstname.lastname&gt;@trixie.res.nrc.gc.ca</li> <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Trixie </li> </ol> </li> <li>Click Save</li> </ol> <p>Once the settings have been saved, you can double click on the name in the list of Saved Sessions to open a session to the Trixie server. Please note that you will be prompted as follows</p> <ol> <li>Prompt for your PUB password</li> <li>LoginTC prompt \u2013 enter 1</li> <li>Prompt for your RES ext password</li> </ol>"},{"location":"External-Access-Advanced-Configuration/#related-topics","title":"Related Topics","text":"<p>External Access Setup Internal Access Setup Internal Access Advanced Configuration File Transfers</p>"},{"location":"External-Access-Setup/","title":"External Access Setup","text":""},{"location":"External-Access-Setup/#overview","title":"Overview","text":"<p>As an external NRC collaborator, you can access the AI for Design (Trixie) Cluster using the Bastion Host. External collaborators include non-NRC researchers, industrial partners, and vendors.</p> <p>You can access only those folders on Trixie that are required for your project. Requests for access to Trixie and specific projects must be made by your NRC research contact; you cannot request access to a system yourself.</p> <p>Once granted access, you will have two sets of credentials issued to access the cluster:</p> Account Purpose User name format (example: John Doe) PUB Provides access to the external bastion host and used for the LoginTC second factor authentication A combination of your first and last name. E.g.: john.doe@pub.nrc-cnrc.gc.ca Trixie System Provides access to Trixie ext.firstname.lastname E.g.: ext.john.doe <p>Your NRC contact, or an NRC system administrator, will provide you with the PUB and ext user names and passwords that you require to access the NRC systems. Note that on first login, you will be required to change your password. Please note: during the password change, the first prompt asks for a confirmation of your existing password prior to requesting a new one.</p>"},{"location":"External-Access-Setup/#logintc-application-setup","title":"LoginTC Application Setup","text":"<p>Before you attempt your first login, the following initial installation and configuration of LoginTC must be implemented.</p> <ul> <li>Upon user creation, you will receive an email to setup and initialize the LoginTC application   (for iOS, Android, or the Chrome web browser) which is used as a second factor authentication   into Trixie</li> <li>Set up LoginTC using the directions provided to you by email</li> </ul>"},{"location":"External-Access-Setup/#accessing-trixie-with-logintc-2-factor-authentication","title":"Accessing Trixie with LoginTC 2-Factor Authentication","text":"<p>In order to access Trixie, you will need to use an SSH client. Please note that you cannot access Trixie using a web browser. On Mac OSX and Linux, SSH is installed by default. On Windows you will need to install Putty if it is not installed already. You can download Putty from the following website:</p> <p>Putty Website</p>"},{"location":"External-Access-Setup/#initialize-ssh-connection-with-mac-osx-linux","title":"Initialize SSH Connection with Mac OSX / Linux","text":"<p>For Mac OSX and Linux you can open a new terminal and connect to <code>trixie.nrc-cnrc.gc.ca</code> via ssh using your PUB account and the following command</p> <p><code>ssh -l &lt;firstname.lastname&gt;@pub.nrc-cnrc.gc.ca trixie.nrc-cnrc.gc.ca</code></p>"},{"location":"External-Access-Setup/#initialize-ssh-connection-with-windows","title":"Initialize SSH Connection with Windows","text":"<p>For Windows, you can create a Putty profile to SSH into the bastion server</p> <p>Under Session</p> <ol> <li>Set Host Name (or IP address): &lt;firstname.lastname&gt;@pub.nrc-cnrc.gc.ca@trixie.nrc-cnrc.gc.ca</li> <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Bastion </li> <li>Click Save</li> </ol> <p>Once the settings have been saved, you can double click on the name in the list of Saved Sessions to open a session to the bastion server.</p>"},{"location":"External-Access-Setup/#logging-in-for-the-first-time","title":"Logging in for the First Time","text":"<p>When you login for the first time you will be forced to change your password for both your PUB account and your Trixie ext account. Please note that when you do this, you will be prompted for your original (or current) password first and then you will be prompted to enter your new password twice.</p> <p>In the following procedure, the information printed in the images may not be the same as what you will see when you login. However the steps will be the same.</p> <p>Please perform the following steps to access Trixie.</p> <ol> <li>When you login using one of the methods above, you will be prompted to authenticate with your    LoginTC application. The message should appear as follows: </li> <li>Press 1 followed by the Enter key and then check your LoginTC device as setup above to    approve the login request</li> <li>If a message similar to the one below appears, then simply type in yes to the prompt as    shown below </li> <li>After you complete the two-factor authentication process in LoginTC you will be prompted to    enter your PUB account password and then you will be forced to change your password. You     should see a message similar to the one below \u2013 remember to enter your original password first    and then enter your new password twice. </li> <li>The system will automatically log you out, thus, you will need to login again using your new    password</li> <li>Once you have successfully logged in, you will be logged into the bastion server \u2013 your screen    should look similar to the following </li> <li>If you have your credentials for the <code>trixie.res.nrc.gc.ca</code> server you can skip this step.    Otherwise, you will now need to contact the administrator who provided you with your credentials    for the bastion server to obtain your credentials for the Trixie server</li> <li>You will need to login to Trixie next. From the bash prompt, use SSH to log into    <code>trixie.res.nrc.gc.ca</code> with your Trixie ext.&lt;firstname.lastname&gt; account and password    with a similar command as the following. <code>ssh ext.&lt;firstname.lastname&gt;@trixie.res.nrc.gc.ca</code></li> <li>If a message similar to the one below appears, then simply type in yes to the prompt as    shown below </li> <li>You will be prompted to enter your Trixie ext account password and then you will be    forced to change your password. You should see a message similar to the one below \u2013 remember to    enter your original password first and then enter your new password twice. </li> <li>The system will automatically log you out, thus, you will need to login again using your new    password</li> <li>Once you have successfully logged in, you will be logged into Trixie \u2013 your screen should look    similar to the following </li> </ol> <p>After successful authentication, you should see the Trixie cluster login banner with terms and be placed in a shell in your home directory on the cluster, similar to the image above.</p> <p>Note that you will be placed in your home directory which only you have access to. For more information on the cluster and its usage, please see the:</p> <p>Home Page</p>"},{"location":"External-Access-Setup/#changing-passwords","title":"Changing passwords","text":"<p>Passwords on the PUB and RES accounts expire after 90 days and must be changed. If you do not change your password, you will be locked out of the system.</p> <p>Watch for the pop-up message notifying you to change your password, or set yourself a reminder to change your password before the 90-day expiry.</p> <p>If you get locked out of your account due to an expired password for any account, notify your NRC contact who can have the password reset.</p>"},{"location":"External-Access-Setup/#change-your-pub-password","title":"Change Your PUB Password","text":"<p>You can change your PUB password by logging into the following website. The site allows you to manage your PUB account. Please use one of the following formats for your username</p> <ul> <li><code>john.doe@pub</code></li> <li><code>doej@pub</code></li> </ul> <p>PUB Account Management</p> <p>Please note that the Reset Password feature will not work if you do not fill in the security questions on the website. Therefore it is strongly recommended that you fill in the security questions so that you can reset your password if necessary.</p>"},{"location":"External-Access-Setup/#change-your-ext-password-via-linux-terminal","title":"Change Your Ext Password via Linux Terminal","text":"<ol> <li>Ensure you are logged into the Trixie server (trixie.res.nrc.gc.ca)</li> <li>Type passwd then hit Enter</li> <li>You will be prompted for your original (or current) password first and then you will be prompted    to enter your new password twice. You should see a message similar to the one below \u2013 remember    to enter your original password first and then enter your new password twice. </li> <li>The system will automatically log you out, thus, you will need to login again using your new    password</li> </ol>"},{"location":"External-Access-Setup/#related-topics","title":"Related Topics","text":"<p>External Access Advanced Configuration Internal Access Setup Internal Access Advanced Configuration File Transfers</p>"},{"location":"External-HPC-Systems/","title":"External HPC Systems","text":""},{"location":"External-HPC-Systems/#overview","title":"Overview","text":"<p>There may be instances where researchers require connectivity to external HPC systems from Trixie. However, network access to and from Trixie is restricted to maintain a high level of security. Therefore, connections to external systems need to be approved before the connection can be opened.</p> <p>This page provides instructions for requesting a connection to an external system, as well as a list of approved systems that already have an open connection.</p>"},{"location":"External-HPC-Systems/#request-a-connection-to-an-external-system","title":"Request a Connection to an External System","text":"<p>In order to submit a request to open a network flow between Trixie and an external HPC system, please post your request in the issues section of this site.</p>"},{"location":"External-HPC-Systems/#approved-external-systems","title":"Approved External Systems","text":"Institution System URL Compute Canada - Cedar cedar.computecanada.ca Compute Canada - Beluga beluga.computecanada.ca Compute Canada - Niagra niagra.computecanada.ca Compute Canada - Graham graham.computecanada.ca Vector Institute v.vectorinstitute.ca NERSC.gov - Cori cori.nersc.gov"},{"location":"File-Transfers/","title":"Transferring Files","text":""},{"location":"File-Transfers/#overview","title":"Overview","text":"<p>This document will describe various procedures for transferring files to and from Trixie.</p> <p>Important Note: For external users, before proceeding with this configuration, please ensure that you have performed the external access setup and advanced configuration procedures.</p>"},{"location":"File-Transfers/#transfers-between-your-local-computer-and-trixie","title":"Transfers Between Your Local Computer and Trixie","text":"<p>The following sections detail how to transfer files between your local computer and Trixie. They basically rely on advanced SSH configurations to bridge the network between your local computer and Trixie.</p>"},{"location":"File-Transfers/#mac-osx-linux","title":"Mac OSX / Linux","text":"<p>To copy a file to the Trixie server, please use the scp command on your local machine.</p>"},{"location":"File-Transfers/#external-users","title":"External Users","text":"<p>Please note that the use of this method requires that your system be configured as detailed in the advanced configuration in order to provide a direct link between your local machine and the Trixie server.</p> <p>The following command will copy the file <code>test.txt</code> from John Doe\u2019s local machine to his ext.john.doe account on Trixie. Please note that using trixie as the hostname will only work if you have configured SSH to use ProxyJump as detailed in the advanced configuration.</p> <p><code>scp test.txt trixie:/home/ext.john.doe</code></p> <p>To copy a file from Trixie to your local machine, you basically reverse the arguments to the scp command.</p> <p><code>scp trixie:/home/ext.john.doe/test.txt test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r myWorkFilesDir trixie:/home/ext.john.doe</code></p>"},{"location":"File-Transfers/#internal-users-from-the-srn-network","title":"Internal Users - From the SRN Network","text":"<p>The following command will copy the file <code>test.txt</code> from John Doe\u2019s local machine to his account on Trixie. Please note that the example assumes the username on Trixie is different than the username on the local machine.</p> <p><code>scp test.txt doej@trixie.res.nrc.gc.ca:/home/doej</code></p> <p>To copy a file from Trixie to your local machine, you basically reverse the arguments to the scp command.</p> <p><code>scp doej@trixie.res.nrc.gc.ca:/home/doej/test.txt test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r myWorkFilesDir doej@trixie.res.nrc.gc.ca:/home/doej</code></p>"},{"location":"File-Transfers/#internal-users-from-the-legacy-network","title":"Internal Users - From the Legacy Network","text":"<p>Please note that the use of this method requires that your system be configured as detailed in the advanced configuration in order to provide a direct link between your local machine and the Trixie server.</p> <p>The following command will copy the file <code>test.txt</code> from John Doe\u2019s local machine to his account on Trixie. Please note that using trixie as the hostname will only work if you have configured SSH to use ProxyJump as detailed in the advanced configuration.</p> <p><code>scp test.txt trixie:/home/doej</code></p> <p>To copy a file from Trixie to your local machine, you basically reverse the arguments to the scp command.</p> <p><code>scp trixie:/home/doej/test.txt test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r myWorkFilesDir trixie:/home/doej</code></p>"},{"location":"File-Transfers/#windows-using-winscp","title":"Windows Using WinSCP","text":"<p>To copy a file to the Trixie server, please use the WinSCP command on your local machine.</p>"},{"location":"File-Transfers/#external-users_1","title":"External Users","text":"<p>If you need to install WinSCP then please download and install it from this site</p> <p>First you will need to configure WinSCP to connect to Trixie using an SSH tunnel. Open WinSCP and follow the procedure below to configure it to access Trixie via an SSH tunnel.</p> <ol> <li>Click the New Session button </li> <li>In the window that pops up, perform the following<ol> <li>Make sure the File protocol is set to SCP</li> <li>Set the Host name: trixie.res.nrc.gc.ca</li> <li>Set the User name: &lt;ext.firstname.lastname&gt;      The window should now look similar to the following </li> <li>Click the Advanced button</li> </ol> </li> <li>In the window that pops up, perform the following<ol> <li>Click the Tunnel item in the left pane</li> <li>Select the Connect through SSH tunnel option</li> <li>Set Host name: trixie.nrc-cnrc.gc.ca</li> <li>Set User name: &lt;username&gt;@pub.nrc-cnrc.gc.ca      The window should now look similar to the following </li> <li>Click the OK button</li> </ol> </li> <li>Click the Save button in the previous popup window</li> <li>In the window that pops up, perform the following<ol> <li>Type in a Site name - perhaps Trixie      The window should now look similar to the following </li> <li>Click the OK button</li> </ol> </li> <li>Click the Login button in the previous popup window    You will be prompted to authenticate with LoginTC (you will need to type 1) and both your Pub and Trixie passwords</li> <li>Once you are logged into your session, you can drag and drop the files you need to transfer between the two file listings</li> </ol>"},{"location":"File-Transfers/#internal-users-from-the-srn-network_1","title":"Internal Users - From the SRN Network","text":"<p>If you need to install WinSCP then please install it from the NRC Software Portal on your desktop.</p> <p>First you will need to configure WinSCP to connect to Trixie. Open WinSCP and follow the procedure below to configure it to access Trixie.</p> <ol> <li>Click the New Session button </li> <li>In the window that pops up, perform the following<ol> <li>Make sure the File protocol is set to SCP</li> <li>Set the Host name: trixie.res.nrc.gc.ca</li> <li>Set the User name: &lt;username&gt;      The window should now look similar to the following </li> <li>Click the Save button</li> </ol> </li> <li>In the window that pops up, perform the following<ol> <li>Type in a Site name - perhaps Trixie      The window should now look similar to the following </li> <li>Click the OK button</li> </ol> </li> <li>Click the Login button in the previous popup window    You will be prompted to authenticate with your Trixie password</li> <li>Once you are logged into your session, you can drag and drop the files you need to transfer between the two file listings</li> </ol>"},{"location":"File-Transfers/#internal-users-from-the-legacy-legacy","title":"Internal Users - From the Legacy Legacy","text":"<p>If you need to install WinSCP then please download and install it from this site</p> <p>First you will need to configure WinSCP to connect to Trixie using an SSH tunnel. Open WinSCP and follow the procedure below to configure it to access Trixie via an SSH tunnel.</p> <ol> <li>Click the New Session button </li> <li>In the window that pops up, perform the following<ol> <li>Make sure the File protocol is set to SCP</li> <li>Set the Host name: trixie.res.nrc.gc.ca</li> <li>Set the User name: &lt;ext.firstname.lastname&gt;      The window should now look similar to the following </li> <li>Click the Advanced button</li> </ol> </li> <li>In the window that pops up, perform the following<ol> <li>Click the Tunnel item in the left pane</li> <li>Select the Connect through SSH tunnel option</li> <li>Set Host name: trixie.nrc-cnrc.gc.ca</li> <li>Set User name: &lt;username&gt;@pub.nrc-cnrc.gc.ca      The window should now look similar to the following </li> <li>Click the OK button</li> </ol> </li> <li>Click the Save button in the previous popup window</li> <li>In the window that pops up, perform the following<ol> <li>Type in a Site name - perhaps Trixie      The window should now look similar to the following </li> <li>Click the OK button</li> </ol> </li> <li>Click the Login button in the previous popup window    You will be prompted to authenticate with LoginTC (you will need to type 1) and both your Pub and Trixie passwords</li> <li>Once you are logged into your session, you can drag and drop the files you need to transfer between the two file listings</li> </ol>"},{"location":"File-Transfers/#windows-using-the-pscp-command-from-putty","title":"Windows Using the pscp Command From Putty","text":"<p>To copy a file to the Trixie server, please use the pscp command on your local machine.</p>"},{"location":"File-Transfers/#external-users-this-process-does-not-work-at-the-moment","title":"External Users - this process does not work at the moment","text":"<p>Please note that the use of this method requires that you have two Putty profiles defined.</p> <ol> <li>A profile for the bastion server</li> <li>A profile for the Trixie server</li> </ol>"},{"location":"File-Transfers/#bastion-server-profile","title":"Bastion Server Profile","text":"<p>The bastion server profile was likely created during the setup configuration for your external access to Trixie. If not, then please see the initialize SSH connection section for detailed instructions on creating a profile for the bastion server.</p>"},{"location":"File-Transfers/#trixie-server-profile","title":"Trixie Server Profile","text":"<p>Follow the procedure below to create the Trixie server profile.</p> <p>Under Session</p> <ol> <li>Set Host Name (or IP address): &lt;ext.firstname.lastname&gt;@trixie.res.nrc.gc.ca</li> <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Trixie-pscp </li> <li>Click Save</li> </ol> <p>Once you have the profiles created and saved, please follow the procedure below to run the pscp command.</p> <ol> <li>Load the Bastion profile and click Open</li> <li>Login to the bastion server and leave the window open</li> <li>Open a Command Prompt window</li> <li>Use the pscp command in the Command Prompt window to copy files to or from the trixie server using the Trixie-pscp putty profile<ol> <li>Copy the file <code>test.txt</code> from John Doe\u2019s local machine to his ext.john.doe account on trixie <code>pscp test.txt Trixie-pscp:/home/ext.john.doe</code></li> <li>To copy a file from trixie to your local machine, you basically reverse the arguments to the pscp command <code>pscp Trixie-pscp:/home/ext.john.doe/test.txt test.txt</code></li> <li>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the pscp command <code>pscp \u2013r myWorkFilesDir Trixie-pscp:/home/ext.john.doe</code></li> </ol> </li> </ol>"},{"location":"File-Transfers/#internal-users","title":"Internal Users","text":"<p>Please note that the use of this method requires that you have a Putty profile defined to access the Trixie server. Follow the procedure below to create the Trixie server profile.</p> <p>Under Session</p> <ol> <li>Set Host Name (or IP address): &lt;username&gt;@trixie.res.nrc.gc.ca</li> <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Trixie-pscp </li> <li>Click Save</li> </ol> <p>Once you have the profile created and saved, please follow the procedure below to run the pscp command.</p> <ol> <li>Open a Command Prompt window</li> <li>Use the pscp command in the Command Prompt window to copy files to or from the trixie server using the Trixie-pscp putty profile<ol> <li>Copy the file <code>test.txt</code> from John Doe\u2019s local machine to his doej account on trixie <code>pscp test.txt Trixie-pscp:/home/doej</code></li> <li>To copy a file from trixie to your local machine, you basically reverse the arguments to the pscp command <code>pscp Trixie-pscp:/home/doej/test.txt test.txt</code></li> <li>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the pscp command <code>pscp \u2013r myWorkFilesDir Trixie-pscp:/home/doej</code></li> </ol> </li> </ol>"},{"location":"File-Transfers/#transfers-between-trixie-and-another-hpc-cluster","title":"Transfers Between Trixie and Another HPC Cluster","text":""},{"location":"File-Transfers/#needs-verification-that-this-is-accurate-and-works-as-well-as-real-parameters-for-the-command-lines-please","title":"Needs verification that this is accurate and works, as well as real parameters for the command lines please","text":"<p>The procedures in this section assume that the advanced SSH configurations discussed above have been implemented. There are three options for copying files between Trixie and another HPC cluster</p> <ol> <li>Copy files directly between Trixie and the HPC cluster</li> <li>Login to Trixie from the other HPC cluster</li> <li>Copy files through your local computer</li> </ol>"},{"location":"File-Transfers/#copy-files-directly","title":"Copy Files Directly","text":"<p>This procedure requires that there is an approved network flow open between Trixie and the second HPC cluster. Please see the external HPC systems page for a list of approved external HPC systems. If there is an approved network flow, then files can be directly copied between Trixie and the second HPC cluster. This is the ideal situation and should be the fastest option in terms of overall network speed between the two systems.</p> <p>To copy a file from the second HPC cluster to Trixie, use the following scp command on the Trixie server.</p> <p><code>scp username@cluster.domain:/home/username/test.txt test.txt</code></p> <p>To copy a file from Trixie to the second HPC cluster, you basically reverse the arguments to the scp command.</p> <p><code>scp test.txt username@cluster.domain:/home/username/test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r myWorkFilesDir username@cluster.domain:/home/username/folder</code></p>"},{"location":"File-Transfers/#login-to-trixie-from-second-cluster","title":"Login to Trixie From Second Cluster","text":"<p>This procedure requires that you have an external account setup to access Trixie. If this is the case, then files can be copied between Trixie and the second HPC cluster via the Bastion Host, but without flowing through your local computer. To use this approach, you will need to login to the second HPC cluster first, and then from the second HPC cluster computer, login to Trixie through the Bastion host.</p> <p>To copy a file from the second HPC cluster to Trixie, use the following scp command on the Trixie server.</p> <p><code>scp username@cluster.domain:/home/username/test.txt test.txt</code></p> <p>To copy a file from Trixie to the second HPC cluster, you basically reverse the arguments to the scp command.</p> <p><code>scp test.txt username@cluster.domain:/home/username/test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r myWorkFilesDir username@cluster.domain:/home/username/folder</code></p>"},{"location":"File-Transfers/#copy-files-through-your-local-computer","title":"Copy Files Through Your Local Computer","text":"<p>This procedure requires that you copy files between the two clusters using your local computer as a bridge. The commands below should be executed on your local computer and not either of the cluster servers.</p> <p>To copy a file from the second HPC cluster to Trixie, use the following scp command on your local computer.</p> <p><code>scp username@cluster.domain:/home/username/test.txt trixie:/home/ext.john.doe/test.txt</code></p> <p>To copy a file from Trixie to the second HPC cluster, you basically reverse the arguments to the scp command.</p> <p><code>scp trixie:/home/ext.john.doe/test.txt username@cluster.domain:/home/username/test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r trixie:/home/ext.john.doe/myWorkFilesDir username@cluster.domain:/home/username/folder</code></p>"},{"location":"File-Transfers/#copy-files-to-a-project-folder","title":"Copy Files to a Project Folder","text":"<p>Project folders have been created for users to use for a couple of purposes:</p> <ol> <li>Storage of data files to use with Trixie. Although you can use your home directory for limited storage of files, it is strongly recommended that you use the project folder instead as there are higher disk quotas for project folders.</li> <li>Sharing of project work and files with team members</li> </ol> <p>Please note that users should be diligent and remove any files and folders (in both the project folder and your home folder) once they are no longer required. This helps to optimize disk usage and avoid disk space issues for all users, not just your own usage.</p> <p>The project folder can be found under the following folder hierarchy</p> <p><code>/gpfs/projects/&lt;project-group&gt;/&lt;project&gt;</code></p> <p>Where project-group is the name of your project group \u2013 for example, AI4D or COVID - and project is the name of your project \u2013 for example, core-01 or bio-01.</p> <p>To copy files to a project folder you should create a personal folder under the project directory and then copy files from your home directory to the new folder. In the example below user John Doe will copy two dataset files to the AI4D/bio-01 project folder.</p> <ol> <li>Change directory to the project folder <code>cd /gpfs/projects/AI4D/bio-01</code></li> <li>Create the new folder using a unique name, perhaps your last name and first initial <code>mkdir doej</code></li> <li>Change back to your home directory <code>cd</code></li> <li>Copy the files to your new project directory <code>cp dataset1.dat dataset2.dat /gpfs/projects/AI4D/bio-01/doej</code></li> </ol>"},{"location":"File-Transfers/#related-topics","title":"Related Topics","text":"<p>External Access Setup External Access Advanced Configuration Internal Access Setup Internal Access Advanced Configuration</p>"},{"location":"Hardware/","title":"Hardware","text":"<p>Trixie is a GPU cluster consisting of 36 nodes, each with NVIDIA V100 GPU, a fast, Infiniband Interconnect, and a large 1 PB global filesystem</p>"},{"location":"Hardware/#operating-system","title":"Operating system","text":"<p>Runs RHEL 9</p>"},{"location":"Hardware/#job-scheduler","title":"Job scheduler","text":"<p>https://slurm.schedmd.com <code>slurm 22.05.9</code> (for example run scripts on Trixie see Running-jobs)</p>"},{"location":"Hardware/#headnode-2-available","title":"Headnode (2 available)","text":"<ul> <li>processor_type = Intel Xeon Gold 6130 CPU clocked at 2.1GHZ 16 cores / CPU</li> <li>processors_per_node = 2</li> <li>RAM = 96 GB memory</li> </ul>"},{"location":"Hardware/#node-profile","title":"Node Profile","text":"<ul> <li>processor_type = Intel Xeon Gold 6130 CPU clocked at 2.1GHZ 16 cores / CPU</li> <li>processors_per_node = 2</li> <li>cores_per_socket = 16</li> <li>threads_per_core = 2 (hyper-threading on)</li> <li>RAM = 192 GB memory</li> <li>GPU_type = NVidia V100 GPUs with 32 GB RAM / GPU</li> <li>GPU_details = https://www.nvidia.com/en-us/data-center/tesla-v100/Nodes</li> <li>GPU_per_node = 4</li> <li>local scratch size =</li> </ul>"},{"location":"Hardware/#filesystem","title":"Filesystem","text":"<ul> <li>1 PB of GPFS storage total, with the following mount points and quotas</li> <li>/home/usernameUser space. Not backed up. Soft quota 1 TB.</li> <li>/gpfs/projects/PROGRAM/project_idShared space for sharing files between users within a project. Each project has a default quota of 1 TB. If additional space is needed   projects can make an official request. Space is not backed up.</li> <li>/gpfs/scratch/   User controlled scratch space for running jobs. Not backed up. Subject to Purge policy (LINK LOST)</li> </ul>"},{"location":"Internal-Access-Advanced-Configuration/","title":"Internal Access Advanced Configuration","text":""},{"location":"Internal-Access-Advanced-Configuration/#overview","title":"Overview","text":"<p>To ease usage of the bastion host to easily connect to Trixie, there are some steps which can be taken, especially making use of the SSH ProxyJump and ControlMaster parameters. Basically, you need to configure SSH to automatically connect with the Trixie server using the bastion host as a connector between your local computer and the Trixie server.</p> <p>Important Note: Before proceeding with this configuration, please ensure that you have performed the Internal Access Setup procedure.</p>"},{"location":"Internal-Access-Advanced-Configuration/#mac-osx-linux","title":"Mac OSX / Linux","text":"<p>To configure SSH to automatically connect to the Trixie server, please perform the following steps</p> <ol> <li>Open your <code>.ssh/config</code> file with your preferred text editor and add the following lines on    your local machine \u2013 not the servers \u2013 while substituting your given usernames in the User    directive. TIP: Some users have reported that using <code>doej</code> did not work for their PUB username.    Please try using the first.last format - for example <code>john.doe</code> - for your PUB username if    <code>doej</code> doesn't work for you</li> </ol> <pre><code>Host trixie-bastion\n  HostName trixie.nrc-cnrc.gc.ca\n  User &lt;username&gt;@pub.nrc-cnrc.gc.ca\n  ForwardAgent true\n\nHost trixie\n  HostName trixie.res.nrc.gc.ca \n  User &lt;username&gt;\n  ProxyJump trixie-bastion\n</code></pre> <p>Once your settings are configured, you will be able to login to the Trixie server with the following command</p> <p><code>ssh trixie</code></p> <p>Please note that you will be prompted as follows</p> <ol> <li>Prompt for your PUB password</li> <li>LoginTC prompt \u2013 enter 1</li> <li>Prompt for your SRN password</li> </ol>"},{"location":"Internal-Access-Advanced-Configuration/#windows-putty","title":"Windows \u2013 Putty","text":"<p>To configure SSH to automatically connect to the Trixie server, please set the following settings in your Putty application, substituting your username where applicable.</p> <ol> <li>Under Connection -&gt; Proxy<ol> <li>From the Proxy type dropdown, select: SSH to proxy and use port forwarding</li> <li>Set Proxy hostname: trixie.nrc-cnrc.gc.ca</li> <li>Set Port: 22</li> <li>Set Username: &lt;username&gt;@pub.nrc-cnrc.gc.ca TIP: Some users have reported that using <code>doej</code> did not work for their PUB username.      Please try using the first.last format - for example <code>john.doe</code> - for your PUB username if      <code>doej</code> doesn't work for you</li> <li>Leave Password blank - do not fill it in </li> </ol> </li> <li>Under Connection -&gt; SSH -&gt; X11<ol> <li>Select the option Enable X11 forwarding </li> </ol> </li> <li>Under Session<ol> <li>Set Host Name (or IP address): &lt;username&gt;@trixie.res.nrc.gc.ca</li> <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Trixie </li> </ol> </li> <li>Click Save</li> </ol> <p>Once the settings have been saved, you can double click on the name in the list of Saved Sessions to open a session to the Trixie server. Please note that you will be prompted as follows</p> <ol> <li>Prompt for your PUB password</li> <li>LoginTC prompt \u2013 enter 1</li> <li>Prompt for your SRN password</li> </ol>"},{"location":"Internal-Access-Advanced-Configuration/#related-topics","title":"Related Topics","text":"<p>Internal Access Setup External Access Setup External Access Advanced Configuration File Transfers</p>"},{"location":"Internal-Access-Setup/","title":"Internal Access Setup","text":""},{"location":"Internal-Access-Setup/#overview","title":"Overview","text":"<p>As an internal NRC employee, you can access the AI for Design (Trixie) Cluster using your SRN credentials. On your first login, you may be required to change your password. Please note: during the password change, the first prompt asks for a confirmation of your existing password prior to requesting a new one.</p> <p>In order to access Trixie, you will need to use an SSH client. Please note that you cannot access Trixie using a web browser. On Mac OSX and Linux, SSH is installed by default. On Windows you will need to install Putty if it is not installed already. You can install Putty from the NRC software portal which should be an icon on your Windows desktop.</p> <p>Trixie can only be accessed from one of two ways</p> <ol> <li>The Secure Research Network (SRN)</li> <li>The Legacy network using the bastion host</li> </ol> <p>The following sections will detail the procedures for logging into Trixie from these networks.</p>"},{"location":"Internal-Access-Setup/#logging-in-from-the-srn-network","title":"Logging in From the SRN Network","text":"<p>In order to access Trixie from the SRN network, you can login to Trixie directly with an SSH client.</p>"},{"location":"Internal-Access-Setup/#initialize-ssh-connection-with-mac-osx-linux","title":"Initialize SSH Connection with Mac OSX / Linux","text":"<p>For Mac OSX and Linux you can open a new terminal and connect to <code>trixie.res.nrc.gc.ca</code> via ssh using your SRN credentials and the following command, where <code>&lt;username&gt;</code> is your SRN user ID consisting of your lastname followed by one or more initials of your first name - like <code>doej</code> for John Doe.</p> <p><code>ssh -l &lt;username&gt; trixie.res.nrc.gc.ca</code></p>"},{"location":"Internal-Access-Setup/#initialize-ssh-connection-with-windows","title":"Initialize SSH Connection with Windows","text":"<p>For Windows, you can create a Putty profile to SSH into Trixie. Note that <code>&lt;username&gt;</code> is your SRN user ID consisting of your lastname followed by one or more initials of your first name - like <code>doej</code> for John Doe.</p> <p>Under Session</p> <ol> <li>Set Host Name (or IP address): &lt;username&gt;@trixie.res.nrc.gc.ca</li> <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Trixie </li> <li>Click Save</li> </ol> <p>Once the settings have been saved, you can double click on the name in the list of Saved Sessions to open a session to Trixie.</p>"},{"location":"Internal-Access-Setup/#logging-into-trixie-a-tutorial","title":"Logging into Trixie - a Tutorial","text":"<p>In the following procedure, the information printed in the images may not be the same as what you will see when you login. However the steps will be the same.</p> <p>Please perform the following steps to access Trixie.</p> <ol> <li>From the terminal prompt, use SSH to log into <code>trixie.res.nrc.gc.ca</code> with your SRN account    and password with a similar command as the following. <code>ssh &lt;username&gt;@trixie.res.nrc.gc.ca</code></li> <li>If a message similar to the one below appears, then simply type in yes to the prompt as    shown below </li> <li>Once you have successfully logged in, you will be logged into Trixie \u2013 your screen should look    similar to the following </li> </ol> <p>NOTE 1: After successful authentication, you should see the Trixie cluster login banner with the terms of use and be placed in a shell in your home directory on the cluster, similar to the image above.</p> <p>NOTE 2: You will be placed in your home directory which only you have access to. For more information on the cluster and its usage, please see the:</p> <p>Home Page</p>"},{"location":"Internal-Access-Setup/#logging-in-from-the-legacy-network","title":"Logging in From the Legacy Network","text":"<p>In order to access Trixie from the Legacy network, you will need to use an SSH client to login to a bastion host with the LoginTC authentication application.</p>"},{"location":"Internal-Access-Setup/#accessing-trixie-with-logintc-2-factor-authentication","title":"Accessing Trixie with LoginTC 2-Factor Authentication","text":"<p>LoginTC is an authentication application that you should have installed on your mobile phone. It implements a two-factor authentication system for secure access to Trixie.</p>"},{"location":"Internal-Access-Setup/#logintc-application-setup","title":"LoginTC Application Setup","text":"<p>Before you attempt your first login, the following initial installation and configuration of LoginTC must be implemented.</p> <ul> <li>You should have received an email to setup and initialize the LoginTC application   (for iOS, Android, or the Chrome web browser)</li> <li>Set up LoginTC using the directions provided to you by email</li> </ul>"},{"location":"Internal-Access-Setup/#initialize-ssh-connection-with-mac-osx-linux_1","title":"Initialize SSH Connection with Mac OSX / Linux","text":"<p>For Mac OSX and Linux you can open a new terminal and connect to <code>trixie.nrc-cnrc.gc.ca</code> via ssh using your PUB account and the following command, where <code>&lt;username&gt;</code> is your SRN user ID consisting of your lastname followed by one or more initials of your first name - like <code>doej</code> for John Doe.</p> <p><code>ssh -l &lt;username&gt;@pub.nrc-cnrc.gc.ca trixie.nrc-cnrc.gc.ca</code></p> <p>TIP 1: Your PUB password is the same password you use to connect to the Legacy VPN.</p> <p>TIP 2: Some users have reported that using <code>doej</code> did not work for their username. Please try using the first.last format - for example <code>john.doe</code> - for your username if <code>doej</code> doesn't work for you</p>"},{"location":"Internal-Access-Setup/#initialize-ssh-connection-with-windows_1","title":"Initialize SSH Connection with Windows","text":"<p>For Windows, you can create a Putty profile to SSH into the bastion server. Note that <code>&lt;username&gt;</code> is your PUB user ID consisting of your lastname followed by one or more initials of your first name - like <code>doej</code> for John Doe.</p> <p>TIP 1: Your PUB password is the same password you use to connect to the Legacy VPN.</p> <p>TIP 2: Some users have reported that using <code>doej</code> did not work for their username. Please try using the first.last format - for example <code>john.doe</code> - for your username if <code>doej</code> doesn't work for you</p> <p>Under Session</p> <ol> <li>Set Host Name (or IP address): &lt;username&gt;@pub.nrc-cnrc.gc.ca@trixie.nrc-cnrc.gc.ca</li> <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Bastion </li> <li>Click Save</li> </ol> <p>Once the settings have been saved, you can double click on the name in the list of Saved Sessions to open a session to the bastion server.</p>"},{"location":"Internal-Access-Setup/#logging-into-trixie-a-tutorial_1","title":"Logging into Trixie - a Tutorial","text":"<p>In the following procedure, the information printed in the images may not be the same as what you will see when you login. However the steps will be the same.</p> <p>Please perform the following steps to access Trixie.</p> <ol> <li>When you login using one of the methods above, you will be prompted to authenticate with your    LoginTC application. The message should appear as follows: </li> <li>Press 1 followed by the Enter key and then check your LoginTC device as setup above to    approve the login request</li> <li>If a message similar to the one below appears, then simply type in yes to the prompt as    shown below </li> <li>Once you have successfully logged in, you will be logged into the bastion server \u2013 your screen    should look similar to the following </li> <li>You will need to login to Trixie next. From the bash prompt, use SSH to log into    <code>trixie.res.nrc.gc.ca</code> with your SRN account and password with a similar command as the    following. <code>ssh &lt;username&gt;@trixie.res.nrc.gc.ca</code></li> <li>If a message similar to the one below appears, then simply type in yes to the prompt as    shown below </li> <li>Once you have successfully logged in, you will be logged into Trixie \u2013 your screen should look    similar to the following </li> </ol> <p>NOTE 1: After successful authentication, you should see the Trixie cluster login banner with the terms of use and be placed in a shell in your home directory on the cluster, similar to the image above.</p> <p>NOTE 2: You will be placed in your home directory which only you have access to. For more information on the cluster and its usage, please see the:</p> <p>Home Page</p>"},{"location":"Internal-Access-Setup/#changing-passwords","title":"Changing passwords","text":"<p>Passwords on the PUB and SRN accounts expire after a certain amount of time and must be changed. If you do not change your password, you could be locked out of the system.</p> <p>Watch for the pop-up message notifying you to change your password, or set yourself a reminder to change your password every so often - perhaps every 90 days.</p>"},{"location":"Internal-Access-Setup/#being-forced-to-change-your-password","title":"Being Forced to Change Your Password","text":"<p>If your password has expired, or you are using a password for the first time, then the system will likely force you to change your password. Please note that you will be prompted for your original (or current) password first and then you will be prompted to enter your new password twice.</p> <p>The following is an example of the system forcing you to change your password</p> <ol> <li>During a login session you will be prompted to enter your password and then the system forces    you to change your password. You should see a message similar to the one below \u2013 remember to    enter your original password first and then enter your new password twice. </li> <li>The system will automatically log you out, thus, you will need to login again using your new    password</li> </ol>"},{"location":"Internal-Access-Setup/#change-your-pub-password","title":"Change Your PUB Password","text":"<p>You can change your PUB password by logging into the following website. The site allows you to manage your PUB account. Please use one of the following formats for your username</p> <ul> <li><code>john.doe@pub</code></li> <li><code>doej@pub</code></li> </ul> <p>PUB Account Management</p> <p>Please note that the Reset Password feature will not work if you do not fill in the security questions on the website. Therefore it is strongly recommended that you fill in the security questions so that you can reset your password if necessary.</p>"},{"location":"Internal-Access-Setup/#change-your-srn-password-via-linux-terminal","title":"Change Your SRN Password via Linux Terminal","text":"<ol> <li>Ensure you are logged into the Trixie server (trixie.res.nrc.gc.ca)</li> <li>Type passwd then hit Enter</li> <li>You will be prompted for your original (or current) password first and then you will be prompted    to enter your new password twice. You should see a message similar to the one below \u2013 remember    to enter your original password first and then enter your new password twice. </li> <li>The system will automatically log you out, thus, you will need to login again using your new    password</li> </ol>"},{"location":"Internal-Access-Setup/#related-topics","title":"Related Topics","text":"<p>Internal Access Advanced Configuration External Access Setup External Access Advanced Configuration File Transfers</p>"},{"location":"Jobs-conda-pytorch/","title":"conda-pytorch","text":"<p>This examples will show you how to setup and prepare an environment for PyTorch jobs using conda on Trixie:</p>"},{"location":"Jobs-conda-pytorch/#1-create-a-pytorch-miniconda-environment","title":"1. Create a pytorch miniconda environment","text":"<p>Either run from the command line or create pytorchconda-environment.sh and run it:</p> <pre><code>#!/bin/bash\n# load the miniconda module\nmodule load conda/3-24.9.0\n# create a conda environment with python 3.7 named pytorch\nconda create --name pytorch python=3.7\nsource activate pytorch\n# install pytorch dependencies via conda\nconda install pytorch==1.7.1 torchvision==0.8.2 cudatoolkit=10.1 -c pytorch\n</code></pre>"},{"location":"Jobs-conda-pytorch/#2-create-a-test-pytorch-python-script-testtorchpy","title":"2. Create a test pytorch python script: testtorch.py","text":"<pre><code>import torch\nprint('GPU available:', torch.cuda.is_available())\n</code></pre>"},{"location":"Jobs-conda-pytorch/#3-create-a-job-submission-script-testpytorchsh","title":"3. Create a job submission script: testpytorch.sh","text":"<pre><code>#!/bin/bash\n\n# Specify the partition of the cluster to run on (Typically TrixieMain)\n#SBATCH --partition=TrixieMain\n# Add your project account code using -A or --account\n#SBATCH --account ai4d\n# Specify the time allocated to the job. Max 12 hours on TrixieMain queue.\n#SBATCH --time=12:00:00\n# Request GPUs for the job. In this case 4 GPUs\n#SBATCH --gres=gpu:4\n# Print out the hostname that the jobs is running on\nhostname\n# Run nvidia-smi to ensure that the job sees the GPUs\n/usr/bin/nvidia-smi\n\n# Load the miniconda module on the compute node\nmodule load conda/3-24.9.0\n# Activate the conda pytorch environment created in step 1\nsource activate pytorch\n# Launch our test pytorch python file\npython testtorch.py\n</code></pre>"},{"location":"Jobs-conda-pytorch/#4-submit-job-for-execution","title":"4. Submit job for execution","text":"<pre><code>sbatch testpytorch.sh\n</code></pre> <p>Output will be 'Submitted batch job XXXXX'</p>"},{"location":"Jobs-conda-pytorch/#5-confirm-execution-results","title":"5. Confirm execution results","text":"<p>Local directory will contain a file 'slurm-XXXXX.out' which is the output of the job (stdout).</p> <p>Output should be:</p> <pre><code>cnXXX - &lt;nodename&gt;\n&lt;Date&gt;\n+--------\n| NVIDIA-SMI XXXX...\n....\n(4 listed V100 GPUs number 0 to 3)\n\nGPU available: True\n</code></pre>"},{"location":"Networking-and-connectivity/","title":"Network Connectivity","text":"<p>This document indicates how users can connect to Trixie as well as other network information.</p>"},{"location":"Networking-and-connectivity/#accessing-trixie","title":"Accessing Trixie","text":"<p>The Trixie head node can be accessed from within NRC via SSH on NRC Legacy (black) and the Secure Research Netowrk (orange).</p> <p>Trixie can also be accessed external to NRC using the Bastion host. Please see the <code>External Access Setup</code> link in the Related Topics section below</p>"},{"location":"Networking-and-connectivity/#outbound-ssh-access-to-external-sites","title":"Outbound SSH Access to External Sites","text":"<p>The Trixie head node has outbound SSH access to a limited number of external sites (e.g. some Canadian Universities). If you require access to an additional site which is not currently available, create a request via our issues page</p>"},{"location":"Networking-and-connectivity/#cloning-git-projects","title":"Cloning Git Projects","text":"<p>You can clone Git projects into your Trixie directory from several Git servers. The table below indicates the current servers and what type of access is enabled.</p> Git Server SSH HTTPS gitlabc Yes Yes - requires access tokens git-collab Broken Yes - requires access tokens gitlab.res Yes Yes github Yes Yes"},{"location":"Networking-and-connectivity/#using-access-tokens-for-gitlabc-and-git-collab","title":"Using Access Tokens for Gitlabc and Git-Collab","text":"<p>MFA is enforced on gitlabc and git-collab, which means you will not be able to pull using HTTPS the standard way. If you do not want to set up SSH keys and would like to pull using HTTPS, the following procedure describes the method for accessing these servers using HTTPS and an access token.</p> <ol> <li>Create an access token on gitlabc or git-collab<ol> <li>Log in to your GitLab account</li> <li>Go to your Profile settings</li> <li>Go to Access tokens</li> <li>Choose a name and optionally an expiry date for the token</li> <li>Choose the desired scopes (api access)</li> <li>Click on Create personal access token</li> <li>Save the personal access token somewhere safe. Once you leave or refresh the page, you    won't be able to access it again.</li> </ol> </li> <li>Clone your Git project <code>git clone https://oauth2:PERSONAL_ACCESS_TOKEN@gitserver.nrc.gc.ca/namespace/projectname.git</code><ul> <li>Replace <code>PERSONAL_ACCESS_TOKEN</code> with the token you have generated</li> <li>Replace <code>gitserver</code> with either <code>gitlabc</code> or <code>git-collab</code></li> <li>Replace <code>namespace</code> with the appropriate group or personal name space</li> <li>Replace <code>projectname</code> with the name of your project</li> </ul> </li> </ol>"},{"location":"Networking-and-connectivity/#related-topics","title":"Related Topics","text":"<p>External Access Setup</p> <p>External Access Advanced Configuration</p>"},{"location":"Running-jobs/","title":"Quickstart","text":"<p>Trixie use the slurm scheduler to manage jobs. Compute Canada has a very good guide for using slurm to submit jobs to a cluster most of which is applicable for Trixie: https://docs.computecanada.ca/wiki/Running_jobs</p> <p>Here is a simple job which runs the python code hello.py</p> <p>Contents of hello.py</p> <pre><code>print('Hello world')\n</code></pre> <p>Contents of hello-job.sh</p> <pre><code>#!/bin/bash\n#SBATCH -J helloworld\n\nmodule load conda/3-24.9.0\nsrun python ~/hello.py\n</code></pre> <p>Submit job:</p> <pre><code>sbatch ./hello-job.sh\n</code></pre> <p>Output will be located in slurm-<code>&lt;jobid&gt;</code>.out</p> <p>In order for a job to run on Trixie, it must be \"billed\" against an approved project. Users are able to charge different projects depending on what their activity is for.</p> <p>See here for the Account Codes</p>"},{"location":"Running-jobs/#more-jobs-examples","title":"More jobs examples","text":"<ul> <li>jobs-conda-pytorch</li> <li>jobs-jupyterlab</li> <li>jobs-conda-RAPIDS</li> <li>jobs-abinit</li> </ul>"},{"location":"SLURM%2C-pytorch-distributed-and-Multiple-Nodes/","title":"Running pytorch.distributed on Multiple Nodes","text":"<p>Key thing to know is that srun is like a super-ssh which means that when running <code>srun cmd</code> it actually does something like <code>ssh node cmd</code></p>"},{"location":"SLURM%2C-pytorch-distributed-and-Multiple-Nodes/#initial-solution","title":"Initial Solution","text":""},{"location":"SLURM%2C-pytorch-distributed-and-Multiple-Nodes/#taskslurm","title":"task.slurm","text":"<pre><code>#!/bin/bash\n\n#SBATCH --partition=TrixieMain\n#SBATCH --account=dt-mtp\n#SBATCH --time=00:20:00\n#SBATCH --job-name=pytorch.distributed\n#SBATCH --comment=\"Helping Harry with pytorch distributed on multiple nodes.\"\n#SBATCH --gres=gpu:4\n##SBATCH --ntasks=2\n\n#SBATCH --wait-all-nodes=1\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=6\n#SBATCH --output=%x-%j.out\n\n\n# USEFUL Bookmarks\n# [Run PyTorch Data Parallel training on ParallelCluster](https://www.hpcworkshops.com/08-ml-on-parallelcluster/03-distributed-data-parallel.html)\n# [slurm SBATCH - Multiple Nodes, Same SLURMD_NODENAME](https://stackoverflow.com/a/51356947)\n\nexport TQDM_MININTERVAL=90\nhead_node_ip=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\nreadonly head_node_ip\nreadonly head_node_port=$(( $SLURM_JOBID % (50000 - 30000 + 1 ) + 30000 ))\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nreadonly srun='srun --output=%x-%j.%t.out'\n\nenv\n\n$srun bash \\\n   task.sh \\\n      $MASTER_ADDR_JOB \\\n      $MASTER_PORT_JOB &amp;\n\nwait\n</code></pre>"},{"location":"SLURM%2C-pytorch-distributed-and-Multiple-Nodes/#tasksh","title":"task.sh","text":"<p>This script will be executed on each node. Note that we are activating the <code>conda</code> environment in this script so that each node/worker can have the proper environment.</p> <pre><code>#!/bin/bash\n\n# USEFUL Bookmarks\n# [Run PyTorch Data Parallel training on ParallelCluster](https://www.hpcworkshops.com/08-ml-on-parallelcluster/03-distributed-data-parallel.html)\n# [slurm SBATCH - Multiple Nodes, Same SLURMD_NODENAME](https://stackoverflow.com/a/51356947)\n\n#module load conda/3-24.9.0\n#source activate molecule\n\nsource /gpfs/projects/DT/mtp/WMT20/opt/miniforge3/bin/activate\nconda activate pytorch-1.7.1\n\nreadonly MASTER_ADDR_JOB=$1\nreadonly MASTER_PORT_JOB=$2\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nenv\n\npython \\\n   -m torch.distributed.launch \\\n      --nproc_per_node=4 \\\n      --nnodes=$SLURM_NTASKS \\\n      --node_rank=$SLURM_NODEID \\\n      --master_addr=$MASTER_ADDR_JOB \\\n      --master_port=$MASTER_PORT_JOB \\\n      main.py \\\n         --batch_size 128 \\\n         --learning_rate 5e-5 &amp;\n\nwait\n</code></pre>"},{"location":"SLURM%2C-pytorch-distributed-and-Multiple-Nodes/#acceleratehuggingface-specific","title":"Accelerate/HuggingFace Specific","text":"<pre><code>#!/bin/bash\n\n# [SLURM/ACCELERATE](https://github.com/huggingface/accelerate/blob/main/examples/slurm/submit_multinode.sh)\n# [MNIST](https://huggingface.co/blog/pytorch-ddp-accelerate-transformers)\n\n#SBATCH --partition=TrixieMain\n#SBATCH --account=dt-mtp\n\n#SBATCH --time=12:00:00\n#SBATCH --job-name=MNIST.distributed\n#SBATCH --gres=gpu:4\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n\n#SBATCH --output=%x-%j.out\n#SBATCH --signal=B:USR1@30\n#SBATCH --requeue\n\n# Requeueing on Trixie\n# [source](https://www.sherlock.stanford.edu/docs/user-guide/running-jobs/)\n# [source](https://hpc-uit.readthedocs.io/en/latest/jobs/examples.html#how-to-recover-files-before-a-job-times-out)\n\nfunction _requeue {\n   echo \"BASH - trapping signal 10 - requeueing $SLURM_JOBID\"\n   date\n   # This would allow to generically requeue any job but since we are using XLM\n   # which is slurm aware, XLM could save its model before requeueing.\n   scontrol requeue \"$SLURM_JOBID\"\n}\n\nif [[ -n \"$SLURM_JOBID\" ]]; then\n   trap _requeue USR1\nfi\n\n\n# Keep a copy of the code in case it changes between runs.\nhead -n 112312 \"$0\" my_huggingface_trainer.py\n\n# Setup your working environment.\nsource setup_env.sh \"\"\n\nexport TQDM_MININTERVAL=90\n# These are required to setup the distributed framework.\nhead_node_ip=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\nreadonly head_node_ip\nreadonly head_node_port=$(( $SLURM_JOBID % (50000 - 30000 + 1 ) + 30000 ))\n\n# Dump the environment.\n( set -o posix ; set )\n\n\nsrun accelerate launch \\\n  --num_processes=$((SLURM_NNODES * SLURM_GPUS_ON_NODE)) \\\n  --num_machines=\"$SLURM_NNODES\" \\\n  --rdzv_backend=c10d \\\n  --main_process_ip=\"$head_node_ip\" \\\n  --main_process_port=\"$head_node_port\" \\\n  --machine_rank=\"$SLURM_NODEID\" \\\n  my_huggingface_trainer.py\n</code></pre>"},{"location":"Slurm-Fairshare/","title":"Slurm Fairshare User Manual","text":""},{"location":"Slurm-Fairshare/#overview","title":"Overview","text":"<p>Fairshare (specifically, Fair-Tree Fairshare) scheduling in Slurm is a mechanism that allows equitable resource allocation among users and accounts based on their historical resource usage. This ensures all users receive a fair opportunity to access cluster resources, promoting a balanced utilization of GPU computational power.</p>"},{"location":"Slurm-Fairshare/#key-concepts","title":"Key Concepts","text":""},{"location":"Slurm-Fairshare/#accounts","title":"Accounts","text":"<ul> <li> <p>Accounts: In Slurm, accounts are used to group users for resource allocation. Each account can have associated priorities and resource limits. In regards to Beatrix, all NRC users are equally prioritized for resource allocation.</p> </li> <li> <p>Fairshare Weighting: Each account is assigned a fairshare factor that influences job prioritization. The factor is determined based on the account's historical usage and current resource limits. For more details on the specifics please see the slurm fair tree website</p> </li> </ul>"},{"location":"Slurm-Fairshare/#partitions","title":"Partitions","text":"<ul> <li> <p>Partitions: These are logical divisions within the cluster, allowing different sets of resources to be allocated for different types of jobs. Partitions can have different configurations, such as node types and resource limits.</p> </li> <li> <p>Here are the partitions available on the Beatrix cluster</p> Partition Time limit (D-HH:MM:SS) Nodes Note Trixiemain* 12:00:00 34 Default partition for all users Trixielong 2-00:00:00 24 Users must be authorized by the user representative JobTesting 6:00:00 2 Job testing partition Preemptible 12:00:00 30 Jobs submitted here may be pre-empted Larus 7-00:00:00 34 Industry SME partition, not available to NRC users </li> </ul>"},{"location":"Slurm-Fairshare/#fair-tree-fairshare-scheduling","title":"Fair-Tree Fairshare Scheduling","text":""},{"location":"Slurm-Fairshare/#how-it-works","title":"How It Works","text":"<ul> <li> <p>Historical Usage: Slurm tracks the historical usage of resources by each account, adjusting priorities based on this data. Accounts using fewer resources within the fairshare window will have a higher priority relative to other users who have used more resources. This calculation is done at multiple levels, such that if accounts A and B are siblings and A has a higher fairshare factor than B, all children of A will have higher fairshare factors than all children of B.</p> </li> <li> <p>Priority Calculation: Job priorities are calculated using a combination of fair-share factors, job age, and other configurable attributes.</p> </li> <li> <p>Dynamic Adjustment: The system dynamically adjusts priorities, ensuring fair distribution of resources over time.</p> </li> </ul>"},{"location":"Temporary-Filesystem-Backups/","title":"Temporary Files System Backup","text":""},{"location":"Temporary-Filesystem-Backups/#overview","title":"Overview","text":"<p>It is possible to retrieve data from automatically generated filesystem backups. The IBM Spectrum Scale (GPFS) system includes the ability to create filesystem snapshots which create temporary backups of data stored in the filesystem. These snapshots can be accessed to retrieve files that may have been accidentally removed.</p> <p>Please see the GPFS Snapshot document for more details on how to retrieve files from the backups.</p>"},{"location":"Trixie-Status/","title":"Current Trixie Operational Status","text":""},{"location":"Trixie-Status/#upcoming-planned-downtime","title":"Upcoming Planned Downtime","text":"<p>All upcoming planned maintenance dates are subject to change to better meet operational needs</p>"},{"location":"Trixie-Status/#2026-q1-maint","title":"2026-Q1-Maint","text":"<p>StartTime=2026-01-12T00:00:00 EndTime=2026-01-14T00:00:00 Duration=2-00:00:00</p>"},{"location":"Trixie-Status/#2026-q2-maint","title":"2026-Q2-Maint","text":"<p>StartTime=2026-04-06T00:00:00 EndTime=2026-04-11T00:00:00 Duration=5-00:00:00</p>"},{"location":"Trixie-Status/#2026-q3-maint","title":"2026-Q3-Maint","text":"<p>StartTime=2026-07-06T00:00:00 EndTime=2026-07-08T00:00:00 Duration=2-00:00:00</p>"},{"location":"Trixie-Status/#2026-q4-maint","title":"2026-Q4-Maint","text":"<p>StartTime=2026-10-05T00:00:00 EndTime=2026-10-07T00:00:00 Duration=2-00:00:00</p>"},{"location":"Trixie-Status/#current-issues-outages","title":"Current Issues / Outages","text":"<ul> <li>[ONGOING] Please note that the Beatrix cluster has scheduled maintenance from 2026-02-02 00:00 Eastern Time to 2026-02-07 00:00ET. The cluster will be unavailable to users during this time window. Jobs will remain in the queues and will resume when the maintenance is over. Veuillez noter qu'une maintenance est pr\u00e9vue sur la grappe de calcul Beatrix du 2026-02-02 \u00e0 00:00 Heure de l'est au 2026-02-07 \u00e0 00:00 HE. La grappe ne sera pas disponible pendant cette p\u00e9riode. Les t\u00e2ches resteront en file d'attente et reprendront une fois la maintenance termin\u00e9e.</li> </ul>"},{"location":"Trixie-Status/#past-events-incidents","title":"Past Events / Incidents","text":"<ul> <li>[RESOLVED] Monday October 6th, 2025 - Wednesday October 8th, 2025 Please note that the Beatrix cluster has scheduled maintenance from 2025-10-06 00:00EDT to 2025-10-08 00:00EDT. The cluster will be unavailable to users during this time window. Jobs will remain in the queues and will resume when the maintenance is over. Veuillez noter qu'une maintenance est pr\u00e9vue sur le cluster Beatrix du 6 octobre 2025 \u00e0 00h00 HAE au 8 octobre 2025 \u00e0 00h00 HAE. Le cluster sera indisponible pendant cette p\u00e9riode. Les t\u00e2ches resteront en file d'attente et reprendront une fois la maintenance termin\u00e9e.</li> <li>[RESOLVED] Monday, July 7th, 2025 - Wednesday, July 9th, 2025 - Please note that there will be a maintenance performed on Beatrix on from July 7th \u2013 7AM to the 9th - 5PM. The cluster will be unavailable to users during this time window. Jobs will remain in the queues and will resume when the maintenance is over. Research Platform Support --- Veuillez noter qu'un entretien sera effectu\u00e9 sur Beatrix du 7 juillet \u00e0 7h au 9 juillet \u00e0 17h. La grappe sera hors d\u2019usage durant cette p\u00e9riode. Les t\u00e2ches resteront en file d'attente et reprendront une fois la maintenance termin\u00e9e. Soutien \u00e0 la plateforme de recherche</li> <li>[RESOLVED] [2025-04-07 -&gt; 2025-04-11] Trixie will be offline for planned maintenance for system update --- Trixie sera hors ligne pour une maintenance planifi\u00e9e pour la mise \u00e0 jour du syst\u00e8me.Trixie will be updated to RHEL 9.5 to address outstanding CVEs and to remain within the support window for RHEL. User software may be impacted, and RPS has provided 4 nodes for testing software stacks prior to the system update. These nodes are in the 'UpdateTest' queue. Several back-end services will be updated as well, but no user impact is expected from these changes. --- Trixie sera mis \u00e0 jour vers RHEL 9.5 afin de corriger les CVE non r\u00e9solus et de respecter la p\u00e9riode de support de RHEL. Les logiciels utilisateurs pourraient \u00eatre impact\u00e9s, et RPS a fourni quatre n\u0153uds pour tester les piles logicielles avant la mise \u00e0 jour du syst\u00e8me. Ces n\u0153uds sont dans la file d'attente \u00ab\u00a0UpdateTest\u00a0\u00bb. Plusieurs services back-end seront \u00e9galement mis \u00e0 jour, mais ces changements n'auront aucun impact sur les utilisateurs.</li> <li>[RESOLVED] [2025-01-24] Temporary reduction in TrixieMain and TrixieLong Queues / R\u00e9duction temporaire des files d'attente TrixieMain et TrixieLong At the request of a NRC project, 24 of Trixie's 36 nodes have been allocated to the use of a single large job/project. There should be no other impact on users. RPS will inform users when the queues have returned to normal. / \u00c0 la demande d'un projet du CNRC, 24 des 36 n\u0153uds de Trixie ont \u00e9t\u00e9 affect\u00e9s \u00e0 l'utilisation d'un seul gros projet/t\u00e2che. Il ne devrait pas y avoir d'autre impact sur les utilisateurs. SPR informera les utilisateurs lorsque les files d'attente seront revenues \u00e0 la normale.</li> <li>[RESOLVED] - Tuesday, October 22nd, 2024 - As a reminder, LoginTC is used as a second authentication service for: Bastion host for external access to Trixie High Performance Computing Clusters, NRC external collab, NetOps. A maintenance period is required to perform system upgrades. Therefore, the LoginTC service will be unavailable on Tuesday, October 22nd, from 3PM to 5PM EDT. Consequently, you will not be able to access the service for which LoginTC provides authentication. Internal access to Trixie will still be available during this time. If you have any questions regarding this maintenance, do not hesitate to communicate with us (rps-spr@nrc-cnrc.gc.ca)</li> <li>[RESOLVED] - Thursday, October 17, 2024 - The Trixie cluster will be shutdown because of a planned electrical outage that will allow RPPM to commission the new emergency power generator. Start date: Thursday, October 17, 6:00 AM EDT. End date: Monday, October 21, 6:00 PM EDT. If you have any further questions, do not hesitate to contact us at your earliest convenience (rps-spr@nrc-cnrc.gc.ca).</li> <li>[RESOLVED] - Friday September 20, 2024 - Yesterday evening, a Centrify outage affected the users ability to connect the to Hartree and Trixie clusters. Opened sessions at the time are also affected and are now stale and should be ended. Currently: Users trying to connect are still experiencing abnormal behaviors. We are still investigating whether the jobs that were running at the time were affected by the outage. KITS is working on fixing the issue and we will inform you as the situation evolves.</li> <li>[RESOLVED] - Friday June 28, 2024 - The Research Platform Support team is currently proceeding with operating system and storage appliance upgrades on the cluster. In order to do the storage appliance upgrade, the cluster will be offline from the 28th of June at 6AM EDT to the afternoon of July the 2nd for a final data synchronization from the old appliance to the new appliance. Please see the email sent out to users Friday, June 21 for important details concerning this change in the Trixie infrastructure. Thank you for your patience. Research Platform Support - Update - Due to ongoing RES VPN issues affecting the upgrade of the Trixie cluster, the return to service has been delayed to end-of-business July 3rd, 2024. - Update - Due to further RES VPN issues today affecting the upgrade of the Trixie cluster, the return to service has been delayed to July 4th, 2024. A notice will be sent when Trixie is back online.</li> <li>[RESOLVED] - Thursday, June 6, 2024 - Trixie Bastion Host Shutdown Notice - For users connecting from the Internet or the Legacy network. This bastion host upgrades will require downtime that is scheduled to start at 7:00AM EDT on June 6th and will conclude at 5PM. Although the bastion hosts will be offline during this time, jobs submitted prior to this maintenance window by users connecting through the bastion host will continue to run normally. If you have any questions or concerns about this upgrade, please let us know. Thank you for your patience. Research Platform Support</li> <li>[RESOLVED] - Tuesday, June 5, 2024 - There is currently a firewall issue resolving some addresses/URLs from the Digital Research Alliance of Canada (formerly Compute Canada) CVMFS mirrors which is affecting the loading of some modules. Please report any outstanding issues on either the issues page or the RPS mailbox.</li> <li>[RESOLVED] - Friday, May 3, 2024 - This downtime is scheduled to start at 2:30PM EDT on Friday May 3rd and will conclude on the evening of Sunday the 5th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this shutdown, please let us know. Thank you for your patience. Research Platform Support</li> <li>[RESOLVED] - Friday, April 12, 2024 - This downtime is scheduled to start at 2:30PM EDT on Friday April 12th and will conclude on the evening of Sunday the 14th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this shutdown, please let us know. Thank you for your patience. Research Platform Support</li> <li>[RESOLVED] - Friday, February 2, 2024 - This downtime is scheduled to start at 2:00PM EST on February 2nd and will conclude on the evening of the 3rd. A notice will be sent out when the downtime is completed, and the cluster is back online.  If you have any questions or concerns about this shutdown, please let us know. Thank you for your patience. Research Platform Support</li> <li>[RESOLVED] - Friday, January 19, 2023 - This downtime is scheduled to start at 2:30 PM EST on Friday January 19th and will conclude on the evening of the 20th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this shutdown, please let us know. Thank you for your patience. Research Platform Support</li> <li>[CANCELLED] - Thursday, December 14th, 2023 - This downtime is scheduled to start at 2:30 PM EST on Thursday the 14th of December and will conclude on the morning of the 15th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this shutdown, please let us know. Thank you for your patience. Research Platform Support</li> <li>[RESOLVED] - Monday, August 28th, 2023 - KITS will be performing a maintenance on the Trixie HPC cluster on Tuesday, August 29th at 6AM EDT. The cluster should be brought back online around 12PM. Jobs with a run time conflicting with the maintenance starting period will stay in the queue and run after the maintenance. A notice will be sent out when the downtime is completed and the cluster is back online.  If you have any questions or concerns please let us know. Research Platform Support: rps-spr@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Wednesday July 19, 2023 - Thursday July 20, 2023 - Please note that RPPM will be shutting down regular power to building M-55 for electrical emergency repairs on July 20th from 6 to 7AM EDT. KITS will therefore be shutting down the Trixie HPC from July 19th at 5 PM to soon after 7AM on the 20th. A notice will be sent out when the downtime is completed and the cluster is back online. If you have any questions or concerns please let us know. IT Operations: ITOperations-OperationsTI@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Monday June 19, 2023 - Wednesday June 21, 2023 - Please note that in support of the new generator installation taking place at building M-55, RPPM will be shutting down power to building M-55, taking place on two consecutive evenings. Trixie HPC will be unavailable during the scheduled period of Monday June 19th 1:00 pm EDT to morning of Wednesday June 21st. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this maintenance or the maintenance schedule please let us know. Thank you for your patience. IT Operations: ITOperations-OperationsTI@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Friday June 9, 2023 - Monday June 12, 2023 - A period of downtime is required for the Trixie HPC due to work on the buildings electrical systems. We will also use this time to perform some routine maintenance and upgrades. This downtime is scheduled to start at 2:00 PM EDT on Friday June 9th and will conclude on the afternoon of Monday June 12th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this maintenance or the maintenance schedule please let us know. Thank you for your patience. IT Operations: ITOperations-OperationsTI@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Monday March 27, 2023 - A period of downtime is required for the Trixie HPC cluster to perform some routine maintenance and upgrades. This downtime is scheduled for the day of Monday March 27th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this maintenance or the maintenance schedule please let us know. Please note that Slurm should stop accepting jobs that would run into the maintenance period. They will be held in the queue until the maintenance period has ended. Thank you for your patience. IT Operations: ITOperations-OperationsTI@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Monday November 28, 2022 - A period of downtime is required for the Trixie HPC cluster to perform some routine maintenance and upgrades. This downtime is scheduled for the day of Monday November 28th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this maintenance or the maintenance schedule please let us know. Thank you for your patience. IT Operations: ITOperations-OperationsTI@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Monday May 30, 2022 - Please note that the Trixie server is currently offline - possibly due to a network issue.</li> <li>[RESOLVED] - Monday April 11 - Wednesday April 13, 2022 (3 days) Please note that Trixie (AI4D HPC cluster) will be unavailable from April 11-13th (3 days) due to a scheduled upgrade. The GPFS file system will be upgraded during this time. If you have any questions or concerns about this maintenance please send an email to ITOperations-OperationsTI@nrc-cnrc.gc.ca. Update - Thursday April 14, 2022: Due to complications with the upgrade of the storage array the scheduled downtime for the AI4D-Trixie cluster has been extended.  There is currently no estimate for when the cluster will return to service but an update will be sent out as soon as there is more information. - Update - Tuesday April 19, 2022 We are returning the AI4D-Trixie cluster to operational status. Unfortunately the storage array is in a degraded state and is only operating with 25% of its normal transfer capacity.  Expect to see slowness from i/o intensive operations. All of the Compute Nodes as well as the Head Node have been re-imaged.  The default operating environment has changed so expect many versions of the software loaded when you login to have changed. This may cause issues with job scripts created for the previous environment. If you experience any issues please let us know (ITOperations-OperationsTI@nrc-cnrc.gc.ca).</li> <li>[RESOLVED] - Tuesday January 25, 2022 - There appears to be several issues with the Trixie HPC that are impacting access through the Bastion Host and general performance on the headnode.  KITS-ITOps is investigating and hopes to resolve the issues as soon as possible.  We will provide an update when we have more information. Update - There is a technical issue with the SSC managed switch due to a recent power outage. Access from Legacy and RES should still function but external access through the Bastion Host is not working. An SSC technician is supposed to be on site tomorrow morning to investigate.</li> <li>[RESOLVED] - Wednesday December 15, 2021 - External access to Trixie is not available. It appears that there is an error with the SSL cert for the external LoginTC URL. When trying to use the LoginTC app on your phone to accept a login request a certificate error appears and the request is never received. Hopefully the issue will be resolved quickly, but access could be offline for a day or two. </li> <li>[RESOLVED] - Wednesday December 15, 2021 - Trixie is currently unavailable and the issue is being investigated.</li> <li>[RESOLVED] - Thursday Dec 2, 2021 - SSH connection to Trixie via the external bastion host are being blocked. Internal NRC network connectivity and Trixie operations continue normally. Investigation of root cause underway.</li> <li>[RESOLVED - downtime completed successfully] - Monday August 23, 2021 - There will be a maintenance period for the Trixe AI4D Cluster on Monday August 23rd starting at 8:00 am EDT.  Access to the cluster will not be possible during the maintenance. The entire day will be reserved for the maintenance but current estimates suggest it will be returned to service by noon.  Maintenance will involve the replacement of a power distribution unit in one of the racks as well as configuration changes on the primary head node. Every effort will be made to preserve the job queue during the maintenance.</li> <li>[RESOLVED - downtime completed successfully] - We are planning a period of scheduled downtime for the Trixie-AI4D cluster on Monday June 28th from 8:00am to 4:00pm EDT.  This will allow a few maintenance tasks to be performed that would interrupt service. These tasks include modifying the partition structure on the primary head node as well as some security patching. - It has become necessary to add a firmware update to this maintenance window for the Mellanox switches. This will cause the GPFS file system to become unavailable forcing us to shutdown the cluster entirely.  All jobs in the queue at the start of the maintenance period will likely be lost. - Due to unforeseen complications the maintenance period must be extended.</li> <li>[RESOLVED - downtime completed successfully] - A period of downtime for the Trixie (AI4D) cluster is being scheduled for Monday, May 17th from 8:00 am - 6:00 pm EDT. Due to a hardware issue on the storage array there will need to be a scheduled maintenance period as per the vendors recommendation. Please note that the nature of the maintenance will require all jobs in the queue to be terminated at the start of the maintenance window.</li> <li>[RESOLVED - nodes back in main queue] - Compute nodes cn110 and cn125 have been taken out of the main queue to troubleshoot GPU issues</li> <li>[RESOLVED - downtime completed successfully] - A period of downtime for the Trixie (AI4D) cluster is being scheduled for Monday, April 19th from 9:00am-3:00pm.  Due to the nature of the maintenance all jobs in the queue will be terminated at the start of the maintenance window.</li> <li>[RESOLVED] - December 17 - We are currently experiencing issues with Trixie head node performance as detailed in #35 Investigation pending.</li> </ul>"},{"location":"Trixie-Status/#notes","title":"Notes","text":""},{"location":"containerServices/","title":"Containers on Beatrix","text":"<p>There are two main container services available on the cluster. Jobs can leverage Singularity/Apptainer container runtime for containers which run best as discrete jobs (e.g., numerical models with specific library requirements that are easier to package into a container vs. setting up in a specific directory tree). For containers exposing a service within the cluster (e.g., a database server) users can use the trixie-containers host to run OCI containers via podman or rootless docker (actually a translation layer to convert docker calls to podman). </p>"},{"location":"containerServices/#requirements","title":"Requirements","text":"<p>Prior to running containers on the clusters users need to request access via a ticket to Research Platform Support. Both Singularity and Podman/Docker require some per-user configuration due the interaction between network accounts and container runtimes.</p>"},{"location":"jobs-abinit/","title":"abinit","text":"<p>This examples will show you how to setup and prepare an environment for Abinit jobs using conda on Trixie:</p>"},{"location":"jobs-abinit/#1-use-existing-miniconda-install-of-abinit","title":"1. Use existing miniconda install of abinit","text":"<pre><code>#!/bin/bash\n# load spack\nmodule load spack\n# load the miniconda module\nmodule load conda/3-24.9.0\n# create a conda environment named abinit\nconda create --name abinit python=3.7\nsource activate abinit\n# install pytorch dependencies via conda\nconda install abinit cudatoolkit=10.1 -c\n</code></pre>"},{"location":"jobs-abinit/#2-copy-create-a-test-abinit-file-si_208files","title":"2. Copy / create a test abinit file: Si_208.files","text":"<pre><code>cd $work_dir; cat &lt;&lt;_EOF_ &gt;Si_208.files\nSi_208.in\nSi_208.out\nSi_208\ntmp\nSi_208\n14-Si.LDA.fhi\n_EOF_\n</code></pre>"},{"location":"jobs-abinit/#3a-create-a-job-submission-script-test_abinitsh","title":"3(a). Create a job submission script: test_abinit.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=8             # &lt;-- request # nodes from cluster partition\n##BATCH --ntasks-per-node=1   # &lt;-- request # tasks per node (default 1 cpu/task)\n#SBATCH --cpus-per-task=8     # &lt;-- request # cpus/task (OMP_NUM_THREADS)\n#SBATCH --mem=24000           # &lt;-- request 24000MB to run this job.\n#SBATCH --time=160            # &lt;-- request secs of wallclock/run.\n#SBATCH -p TrixieMain         # &lt;-- partition\n#\nexport HOSTFILE=\"/tmp/hosts.$SLURM_JOB_ID\"\nsrun hostname -s &gt; $HOSTFILE\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n# --- Set the working directory using full path ---\nwork_dir=\"/gpfs/share/opt/apps/abinit/examples/Si_208\"\n# --- Set the input filename for the abinit program (full path)\nfiles=\"$work_dir/Si_208.files\"\n\necho \"Loading.. \"\necho spack load miniconda3\nspack load miniconda3@4.8.2\necho . activate abinit\n. activate /gpfs/share/opt/apps/abinit/conda/envs/abinit\necho which abinit\nwhich abinit\n\n#  Set the abinit program using full path:\nbin=`which abinit`\nif [ $? -ne 0 ]; then\n    echo \"abinit program not found!\"\n    exit 1\nfi\nif [ ! -x $bin ]; then\n    echo \"$bin not found!\"\n    exit 1\nfi\n\n# --- Sleep to make sure loaded properly on all nodes ---\nsleep 10\n\n# --- Start MPI jobs ---\nset -x\nwhich mpirun\n\nif [ -z \"$SLURM_NPROCS\" ]; then\n   if [ -z \"$SLURM_NTASKS_PER_NODE\" ]; then\n        SLURM_NTASKS_PER_NODE=1\n   fi\n   SLURM_NPROCS=$[ $SLURM_JOB_NUM_NODES * $SLURM_NTASKS_PER_NODE ]\nfi\n\nmpirun -f $HOSTFILE -n $SLURM_NPROCS -launcher ssh -wdir $work_dir $bin &lt; $files\n\ntest -r $HOSTFILE &amp;&amp; rm $HOSTFILE\n# -----------------------------------------------------------------------------\n</code></pre>"},{"location":"jobs-abinit/#3b-or-module-load-abinit-from-cc-stack","title":"3(b). OR Module load abinit from CC stack","text":"<pre><code>$ module avail abinit\n\n-------------------------------- MPI-dependent avx512 modules ---------------------------------\n   abinit/8.4.4 (chem)    abinit/8.10.2 (chem)    abinit/9.2.1 (chem,D)\n\n  Where:\n   chem:  Chemistry libraries/apps / Logiciels de chimie\n   D:     Default Module\n\n$ module load abinit/8.10.2\n</code></pre> <pre><code>#\n#  abinit_template_slurm.job\n#\n#  Set scheduler parameters\n# -----------------------------------------------------------------------------\n#  --- Send email to address defined below when job is completed or aborted ---\n#SBATCH --mail-type=abort,end\n#\n#  --- Please replace with your email ---\n#SBATCH --mail-user=First.Lastname@cnrc-nrc.gc.ca\n#\n#  The next options define the running environment requested for this job.\n# -----------------------------------------------------------------------------\n#SBATCH --nodes=8             # &lt;-- request # nodes from cluster partition\n#SBATCH --ntasks-per-node=1   # &lt;-- request # tasks per node (default 1 cpu/task)\n#SBATCH --cpus-per-task=8     # &lt;-- request # cpus/task (OMP_NUM_THREADS)\n#SBATCH --mem=24000           # &lt;-- request 24000MB to run this job.\n#SBATCH --time=160            # &lt;-- request secs of wallclock/run.\n#\n#SBATCH -p TrixieMain         # &lt;-- partition\n#\n# -----------------------------------------------------------------------------\n#\n\n# --- Set the working directory using full path ---\nwork_dir=\"/gpfs/share/opt/apps/abinit/examples/Si_208\"\n# --- Set the input filename for the abinit program (full path)\nfiles=\"$work_dir/Si_208.files\"\n\nmodule load abinit/8.10.2\n\nbin=`which abinit`\n\n\nif [ -z \"$SLURM_NPROCS\" ]; then\n   if [ -z \"$SLURM_NTASKS_PER_NODE\" ]; then\n        SLURM_NTASKS_PER_NODE=1\n   fi\n   SLURM_NPROCS=$[ $SLURM_JOB_NUM_NODES * $SLURM_NTASKS_PER_NODE ]\nfi\n\nmpirun -n $SLURM_NPROCS -wdir $work_dir $bin &lt; $files\n\n</code></pre> <p>CC stack cannot yet module load abinit/9.2.1 (default module), as the StdEnv/2020 and libxc/5.0.0 are required for that module. There is no abinit/9.2.1 built using gcc/9.3.0, rather only intel compiler. Instead load the current supported version tested on trixie:</p> <p>Attempt to load abinit/9.2.1 w/ older env:</p> <pre><code>$ module spider abinit/9.2.1\n$ module load       nixpkgs/16.09  intel/2018.3  openmpi/3.1.4\n$ module load abinit/9.2.1\n$ which abinit\n/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx512/MPI/intel2018.3/openmpi3.1/abinit/9.2.1/bin/abinit\n\n</code></pre>"},{"location":"jobs-abinit/#4-submit-job-for-execution","title":"4. Submit job for execution","text":"<pre><code>sbatch test_abinit.sh\n</code></pre> <p>Output will be 'Submitted batch job XXXXX'</p>"},{"location":"jobs-abinit/#5-confirm-execution-results","title":"5. Confirm execution results","text":"<p>Local directory will contain a file 'slurm-XXXXX.out' which is the output of the job (stdout).</p> <p>Output should be:</p> <pre><code>cnNNN\ncnNNN\n..\nLoading..\nspack load miniconda3@4.8.2\n. activate abinit\nwhich abinit\n/gpfs/share/opt/apps/abinit/conda/envs/abinit/bin/abinit\n+ which mpirun\n/gpfs/share/opt/apps/abinit/conda/envs/abinit/bin/mpirun\n+ mpirun -f /tmp/hosts.61574 -n 8 -launcher ssh -wdir /gpfs/share/opt/apps/abinit/examples/Si_208 /gpfs/share/opt/apps/abinit/conda/envs/abinit/bin/abinit\n  ABINIT 9.0.4\n</code></pre>"},{"location":"jobs-conda-RAPIDS/","title":"conda-RAPIDS","text":"<p>This examples will show you how to setup and prepare an environment for RAPID jobs using conda on Trixie:</p>"},{"location":"jobs-conda-RAPIDS/#1-create-a-rapids-miniconda-environment-copy-and-paste-the-next-3-sections-in-a-terminal-execute-individually","title":"1. Create a rapids miniconda environment. Copy and paste the next 3 sections in a terminal. Execute individually","text":"<pre><code># ONLY USE IF YOU NEED TO REMOVE THE Python ENV (commented for safety) &amp;&amp; \\\n# ==================================================================== &amp;&amp; \\\n#conda deactivate &amp;&amp; \\\n#conda remove --name rapids --all\n</code></pre> <pre><code># Load Modules and setup Python ENV &amp;&amp; \\\n# ================================= &amp;&amp; \\\n# Some of these modules may not be necessary &amp;&amp; \\\n# You will have to do your own testing if you want a minimal &amp;&amp; \\\n# module selection &amp;&amp; \\\nmodule purge  &amp;&amp; \\\nmodule load conda/3-24.9.0 &amp;&amp; \\\nmodule load python-3.8.3-gcc-9.2.0-qxa3ikk &amp;&amp; \\\nmodule load cuda-10.1.168-gcc-9.2.0-xyykr4t &amp;&amp; \\\nmodule load gcc-9.2.0-gcc-9.2.0-vzr5o5q &amp;&amp; \\\nmodule load autoconf-2.69-gcc-9.2.0-qbibewz &amp;&amp; \\\nmodule load automake-1.16.2-gcc-9.2.0-6m76nfe &amp;&amp; \\\nconda create -n rapids python=3.8  &amp;&amp; \\\nconda activate rapids\n</code></pre> <pre><code># Install RAPIDS in rapids ENV &amp;&amp; \\\n# ============================ &amp;&amp; \\\nconda install -c rapidsai -c nvidia -c conda-forge \\\n    -c defaults rapids=0.15 python=3.8 cudatoolkit=10.1\n</code></pre>"},{"location":"jobs-conda-RAPIDS/#2-create-a-test-rapids-python-script-test_rapidspy","title":"2. Create a test RAPIDS python script: test_rapids.py","text":"<pre><code>import cudf\nimport numpy as np\n\ns = cudf.Series([1,2,3,None,4])\nprint(s)\n\ndf = cudf.DataFrame({'a': list(range(20)),\n                     'b': list(reversed(range(20))),\n                     'c': list(range(20))\n                    })\nprint(df.head(20))\n\n\nprint()\nprint()\n\n# Simple UDF\ndef simple_udf(x):\n    if x &gt; 0:\n        return x + 5\n    else:\n        return x - 5\n\n\n# Should run on GPU\nprint(df['a'].applymap(simple_udf))\n\nprint('... done ...')\n</code></pre>"},{"location":"jobs-conda-RAPIDS/#3-create-a-job-submission-script-test_rapidsslurm","title":"3. Create a job submission script: test_rapids.slurm","text":"<pre><code>#!/bin/bash\n# This script was modified for RAPIDS testing purposes...\n# Original script : https://github.com/ai4d-iasc/trixie/wiki/Jobs-conda-pytorch\n# Specify the partition of the cluster to run on\n#SBATCH --partition=JobTesting\n# Add your project account code using -A or --account\n#SBATCH --account ai4d-bio-04c\n# Specify the time allocated to the job. (30 mins just for kicks)\n#SBATCH --time=00:30:00\n# Request GPUs for the job. In this case 1 GPU\n#SBATCH --gres=gpu:1\n# Print out the hostname that the jobs is running on\nhs=`hostname`\necho -e \"host name : $hs\"\necho -e \"\\n\"\necho -e \"\\n\"\n\n# Run nvidia-smi to ensure that the job sees the GPUs\n/usr/bin/nvidia-smi\n\necho -e \"\\n\"\necho -e \"\\n\"\n\n# Load the miniconda module on the compute node\nmodule load conda/3-24.9.0\nmodule load python-3.8.3-gcc-9.2.0-qxa3ikk\nmodule load cuda-10.2.89-gcc-9.2.0-fa5atrg\nmodule load gcc-9.2.0-gcc-9.2.0-vzr5o5q\nmodule load autoconf-2.69-gcc-9.2.0-qbibewz\nmodule load automake-1.16.2-gcc-9.2.0-6m76nfe\nsource activate rapids\n\n# Launch our test python file\npython test_rapids.py\n</code></pre> <p>Output will be 'Submitted batch job XXXXX'</p>"},{"location":"jobs-conda-RAPIDS/#5-confirm-execution-results","title":"5. Confirm execution results","text":"<p>Local directory will contain a file 'slurm-XXXXX.out' which is the output of the job (stdout).</p> <p>Output should be:</p> <pre><code>host name : cn135\n\n\n\n\nThu Sep 24 17:00:50 2020\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n| N/A   31C    P0    42W / 300W |      0MiB / 32480MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\n\n0       1\n1       2\n2       3\n3    &lt;NA&gt;\n4       4\ndtype: int64\n\n     a   b   c\n0    0  19   0\n1    1  18   1\n2    2  17   2\n3    3  16   3\n4    4  15   4\n5    5  14   5\n6    6  13   6\n7    7  12   7\n8    8  11   8\n9    9  10   9\n10  10   9  10\n11  11   8  11\n12  12   7  12\n13  13   6  13\n14  14   5  14\n15  15   4  15\n16  16   3  16\n17  17   2  17\n18  18   1  18\n19  19   0  19\n\n\n0     -5\n1      6\n2      7\n3      8\n4      9\n5     10\n6     11\n7     12\n8     13\n9     14\n10    15\n11    16\n12    17\n13    18\n14    19\n15    20\n16    21\n17    22\n18    23\n19    24\nName: a, dtype: int64\n... done ...\n</code></pre>"},{"location":"jobs-jupyterlab/","title":"Experimentation - Jupyter Notebook","text":"<p>At the end of this tutorial you will have:</p> <ul> <li>created a \"env\" directory inside a project directory with:</li> <li>an environment running keras using CPU(s)</li> <li>an environment running keras using GPU(s)</li> <li>defined 2 slurm jobs starting jupyter making use of respective environment</li> </ul>"},{"location":"jobs-jupyterlab/#creating-conda-environments","title":"Creating conda environments","text":"<pre><code>#load conda module\nmodule load conda/3-24.9.0\n\n#define a project variable\nexport MY_PROJECT_ROOT=$HOME/sample_project\n\n#go to project root\ncd $MY_PROJECT_ROOT\n\n#create a directory for all the environments\nmkdir env\n\n#create the CPU variant\nconda create -c conda-forge -p $MY_PROJECT_ROOT/env/Covid-Net-cpu python=3 jupyterlab imutils opencv\nmatplotlib keras scikit-learn pandas\n\n#create the GPU variant\nconda create -c conda-forge -p $MY_PROJECT_ROOT/env/Covid-Net-gpu python=3 jupyterlab imutils opencv\nmatplotlib keras scikit-learn pandas tensorflow-gpu\n\n#create a directory to hold jobfiles\nmkdir $MY_PROJECT_ROOT/jobs\n</code></pre>"},{"location":"jobs-jupyterlab/#example-of-a-my_project_rootjobsjupyter-cpujob-file","title":"example of a $MY_PROJECT_ROOT/jobs/jupyter-cpu.job file","text":"<pre><code>#!/bin/bash -l\n#SBATCH --account=covid-01\n#SBATCH --partition=TrixieMain\n#SBATCH --time=04:00:00 ####MAXIMUM 48:00:00 on Trixie\n#SBATCH --job-name=My_Awesome_Jupyter.cpu ####Try to be a bit descriptive or use the comment if you prefer shorter job names\n##SBATCH --comment=\"Comment on job\" ####Optional comment\n#SBATCH --mem=5G\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=2\n#SBATCH --output=%x-%j.out\n\n##### To help debugging\n#set -x\n\nexport MY_PROJECT_ROOT=$HOME/sample_project\n\nmodule load conda/3-24.9.0\n\nsource activate $MY_PROJECT_ROOT/env/Covid-Net-cpu\njupyter-lab --ip=*\n</code></pre>"},{"location":"jobs-jupyterlab/#example-of-a-my_project_rootjobsjupyter-gpujob-file","title":"example of a $MY_PROJECT_ROOT/jobs/jupyter-gpu.job file","text":"<pre><code>#!/bin/bash -l\n#SBATCH --account=covid-01\n#SBATCH --partition=TrixieMain\n#SBATCH --gres=gpu:1\n#SBATCH --time=04:00:00 ####MAXIMUM 48:00:00 on Trixie\n#SBATCH --job-name=My_Awesome_Jupyter.gpu ####Try to be a bit descriptive or use the comment if you prefer shorter job names\n##SBATCH --comment=\"Comment on job\" ####Optional comment\n#SBATCH --mem=5G\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=2\n#SBATCH --output=%x-%j.out\n\n##### To help debugging\n#set -x\n\nexport MY_PROJECT_ROOT=$HOME/sample_project\n\nmodule load conda/3-24.9.0\n\nsource activate $MY_PROJECT_ROOT/env/Covid-Net-gpu\n\njupyter-lab --ip=*\n</code></pre>"},{"location":"jobs-jupyterlab/#scheduling-a-gpu-enabled-jupyter-notebook","title":"Scheduling a GPU enabled Jupyter notebook","text":"<pre><code>#starting a GPU enabled Jupyter notebook\nsbatch $MY_PROJECT_ROOT/jobs/jupyter-gpu.job\nSubmitted batch job 4502\n\n#you will get a jobid look for your log file names based on the job-name and the jobid\ntail -f My_Awesome_Jupyter.gpu-4502.out\n\n#look for the Jupyter notebook output you will get the node it is running on and the port number it is listening on. for example:\n#[...]\nTo access the notebook, open this file in a browser:\nfile:///gpfs/home/paulp/.local/share/jupyter/runtime/nbserver-24429-open.html\nOr copy and paste one of these URLs:\nhttp://cn122:8888/?token=388199bb6ef0cad54ef195f1286301548fc15ec2a39eee3c\nor http://127.0.0.1:8888/?token=388199bb6ef0cad54ef195f1286301548fc15ec2a39eee3c\n\n#At that moment a GPU enabled Jupyter notebook is running on compute node \"cn122\" on port 8888\n#Using an ssh tunnel 8888:cn122:8888 (using the following ssh command from the hn2 command line prompt)\nssh -Y -R 8888:cn122:8888 hn2\n#would allow you to access the remote jupyter notebook by connecting to\n#http://127.0.0.1:8888/?token=388199bb6ef0cad54ef195f1286301548fc15ec2a39eee3c\n</code></pre>"},{"location":"jobs-jupyterlab/#releasing-resources-for-others-to-use","title":"Releasing Resources for others to use","text":"<pre><code>#Don't forget to release resources when done by canceling your job\nscancel 4502\n</code></pre>"},{"location":"jobs-jupyterlab/#sample-python-script-that-shows-devices-available-to-use-by-tensorflow-220","title":"sample python script that shows devices available to use by tensorflow 2.2.0","text":"<pre><code>import tensorflow as tf\n\n#display tensorflow version\nprint(tf.__version__)\n\nfrom tensorflow.python.client import device_lib\n\n#output tensorflow devices\nprint(device_lib.list_local_devices())\n</code></pre>"},{"location":"jobs-jupyterlab/#useful-tip","title":"Useful tip","text":"<ul> <li>check on the queue using \"squeue\" or \"squeue -u $USER\" - a running job will have the \"R\" state a job waiting to run will show \"PD\" in the state column</li> <li>you can as some information on the partitions using \"sinfo\"</li> <li>you can get some details on accounting using \"sacct\"</li> <li>The Terminal within jupyter-lab runs within your job so it is safe to use to perform monitoring or other tasks that could be bothersome to others if ran on the head node. If you intend to use a lot of resource like this consider raising how many cores/cpus you request and maybe the RAM also...</li> <li>a nice one liner to run a monitoring task for the GPU, this command will run indefinitely each second and output the listed parameters.</li> </ul> <pre><code>nvidia-smi --query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max,pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 1\n</code></pre>"},{"location":"jobs-jupyterlab/#making-a-ssh-tunnel-straight-to-the-node","title":"Making a SSH Tunnel Straight to the Node","text":"<p>This is somewhat of a rehash of the previous section but here's another way by creating a tunnel straight to the node.</p>"},{"location":"jobs-jupyterlab/#scientificpythonstack","title":"ScientificPythonStack","text":"<p><code>ScientificPythonStack</code> is essentially a community suggested set of commonly used python tools/libraries for scientific work.</p> <pre><code>pip install numpy scipy matplotlib ipython pandas sympy jupyterlab notebook\n</code></pre>"},{"location":"jobs-jupyterlab/#slurm-job-jupyterslurm","title":"SLURM job <code>jupyter.slurm</code>","text":"<p>This is an example <code>slurm</code> script to start a job running <code>jupyter</code>. It uses 6 CPU cores and 16G or RAM. In its current state, the following script doesn't ask for a GPU. If you need a GPU, uncomment <code>##SBATCH --gres=gpu:1</code> to <code>#SBATCH --gres=gpu:1</code>.</p> <pre><code>#!/bin/bash\n# vim:nowrap:\n\n#SBATCH --job-name=Jupyter\n#SBATCH --comment=\"Jupyter Notebook\"\n\n# On Trixie\n#SBATCH --partition=JobTesting\n#SBATCH --account=dt-mtp\n\n##SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=6\n#SBATCH --mem=16G\n#SBATCH --open-mode=append\n#SBATCH --requeue\n#SBATCH --signal=B:USR1@30\n#SBATCH --output=%x-%j.out\n\n# Fix SLURM environment variables.\nSLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE%%(*)}   # '24(x2)' =&gt; 24\nSLURM_TASKS_PER_NODE=${SLURM_TASKS_PER_NODE%%(*)}   # '4(x2)' =&gt; '4'\n\n# NOTE: We set OMP_NUM_THREADS or else we get the following Warning:\n# WARNING:torch.distributed.run:\n# *****************************************\n# Setting OMP_NUM_THREADS environment variable for each process to be 1 in\n# default, to avoid your system being overloaded, please further tune the\n# variable for optimal performance in your application as needed.\n# *****************************************\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-$(nproc)}\n\nsource /gpfs/projects/DT/mtp/venv/ScientificPythonStack/bin/activate \"\"\n\njupyter notebook --no-browser --port 8889  --ip=*\n</code></pre> <p>Then start the notebook.</p> <pre><code>sbatch jupyter.slurm\n</code></pre> <p>If you need a GPU, you can start your job using:</p> <pre><code>sbatch jupyter.slurm --gres=gpu:1\n</code></pre> <p>You will need to know on what node your job is running in order to properly setup a ssh tunnel to it. Use <code>squeue --user $USER</code> to get the worker node name.</p> <pre><code>   JOBID NAME                               USER ST       TIME  NODES NODELIST(REASON) SUBMIT_TIME        COMMENT\n   90324 Jupyter                         larkins  R       0:06      1 trixie-cn101     2024-11-21T10:06:2 Jupyter\n</code></pre>"},{"location":"jobs-jupyterlab/#ssh-tunnel","title":"SSH Tunnel","text":""},{"location":"jobs-jupyterlab/#putty","title":"Putty","text":"<p>To create a tunnel with <code>Putty</code> replacing <code>cnXXX</code> with your worker's hostname (trixie-cnXXX):</p> <p></p> <ul> <li>right click putty's title bar</li> <li>Change Settings...</li> <li>Connection &gt; SSH&gt; Tunnels</li> <li>Source port <code>8889</code></li> <li>Destination <code>trixie-cnXXX:8889</code></li> <li>CLICK ADD</li> <li>then <code>Apply</code></li> </ul>"},{"location":"jobs-jupyterlab/#ssh","title":"ssh","text":"<pre><code>ssh -L 8889:trixie-cnXXX:8889 trixie.res.nrc.gc.ca\n</code></pre>"},{"location":"jobs-jupyterlab/#access-your-jupyter-notebook","title":"Access your Jupyter Notebook","text":"<p>You can look at the log <code>tail -f Jupyter*.out</code> to grab your jupyter's url which has a secret token. This url looks like <code>http://localhost:8889/tree?token=f3e7dee8b55e94912fb4769b4cb0896cc57eeaa1fb7b317d</code>. Using your favorite browser, use the url to access your Jupyter Notebook.</p>"},{"location":"jobs-jupyterlab/#finally-stop-your-worker","title":"Finally Stop your Worker","text":"<p>It is IMPORTANT to <code>scancel &lt;JOBID&gt;</code> when you are not using your jupyter notebook.</p>"},{"location":"jobs-python-virtualenv/","title":"python venv","text":"<p>This examples will show you how to setup and prepare an environment for Python tensorflow jobs using virtualenv on Trixie:</p>"},{"location":"jobs-python-virtualenv/#1-create-a-python-virtual-environment","title":"1. Create a python virtual environment:","text":"<p>Either run from the command line or create tf-py37-environment.sh and run it:</p> <p>file: tf-py37-environment.sh:</p> <pre><code>#!/bin/bash\n\n## 1.(a) load the CC python3.7 module &amp; required libs:\nmodule load python/3.7\nmodule load cuda/10.0 cudnn\nmodule load hdf5\n\n## -or- 1.(b) choose to load python3.8 instead:\n#module load python/3.8\n#module load cuda/10.1.243 cudnn\n#module load hdf5\n\n\n## 2. Create a path for venv\nmkdir -p ~/work/venv\n# or use the project path to share virtualenv:\n#mkdir -p ~/project/venv\n\n## 2.2. Confirm python version &amp; paths (to make sure modules loaded ok:\nmodule list\nwhich python\nwhich pip\npython --version\n\n## 3.1. Create a new virtual environment named tf-py37:\nvirtualenv ~/work/venv/tf-py37\n\n## 3.2. activate env:\nsource ~/work/venv/tf-py37/bin/activate  \n\n## 3.3. run pip to install tf 1.x:\npip install tensorflow-gpu==1.15.0\n\n## See also table: https://stackoverflow.com/questions/50622525/which-tensorflow-and-cuda-version-combinations-are-compatible\n# Trixie presently supports cuda versions: 10.0.x and 10.1.x:\n#   - TF 1 works with cuda 10.0.130\n#   - TF 2 works with cuda 10.1.243\n</code></pre> <p>NOTE: with virtualenv it is still first necessary (upon login/beginning of scripts) to <code>module load</code> for the python version, if it differs from defaults already loaded (in output of: <code>module list</code>).</p>"},{"location":"jobs-python-virtualenv/#2-create-a-test-python-virtualenv-script","title":"2. Create a test python virtualenv script","text":"<p>test-tf-py37.py:</p> <pre><code>import tensorflow as tf\nprint('Tensorflow version:', tf.__version__())\nif tf.test.gpu_device_name():\n  print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nelse:\n  print(\"Could not find GPU. Please install GPU version of TF.\")\n</code></pre>"},{"location":"jobs-python-virtualenv/#3-create-a-job-script","title":"3. Create a job script","text":"<p>test-tf-py37.sh:</p> <pre><code>#!/bin/bash\n\n# Specify the partition of the cluster to run on (Typically TrixieMain)\n#SBATCH --partition=TrixieMain\n# Add your project account code using -A or --account\n#SBATCH --account ai4d\n# Specify the time allocated to the job. Max 12 hours on TrixieMain queue.\n#SBATCH --time=12:00:00\n# Request GPUs for the job. In this case 1 GPU\n#SBATCH --gres=gpu:1\n# Print out the hostname that the jobs is running on\nhostname\n# Run nvidia-smi to ensure that the job sees the GPUs\n/usr/bin/nvidia-smi\n\nmodule load python/3.7\nmodule load cuda/10.0 cudnn\nmodule load hdf5\n\nsource ~/work/venv/tf-py37/bin/activate  \n\n# Launch our test tensorflow python file\npython test-tf-py37.py\n</code></pre>"},{"location":"jobs-python-virtualenv/#4-submit-job-for-execution","title":"4. Submit job for execution:","text":"<p>Remember to run the job script on a compute node using srun or sbatch commands in order to first allocate gpu resources.  </p> <pre><code>sbatch test-tf-py37.sh\n\nsrun --gres=gpu:1 -n 1 --pty /bin/bash --login   \n$ test-tf-py37.sh\n</code></pre> <p>Output will be 'Submitted batch job XXXXX'</p>"},{"location":"jobs-python-virtualenv/#5-confirm-execution-results","title":"5. Confirm execution results:","text":"<p>Local directory will contain a file 'slurm-XXXXX.out' which is the output of the job (stdout).</p> <p>Output should be:</p> <pre><code>cnXXX - &lt;nodename&gt;\n&lt;Date&gt;\n+--------\n| NVIDIA-SMI XXXX...\n....\n(4 listed V100 GPUs number 0 to 3)\n\nTensorflow version: 1.15.0\nDefault GPU Device:  ..\n</code></pre>"},{"location":"quota/","title":"Disk Quotas","text":"<p>Users may have several quotas to manage on Trixie. At the least, each user has their own home and work disk and inode (i.e., number of files and directories) quotas. As well, depending upon the number of research groups they belong to, they will also have disk quotas under <code>/projects</code> for each group to which they belong.</p> <p>When you log to Trixie, you will see your disk quota as part of the login message.</p> <pre><code>Home Quota Disk Usage: 41.4%; Home Quota Inode Usage: 37.0%\nWork Quota Disk Usage: 22.4%; Work Quota Inode Usage: 16.7%\nCheck quota limits with gpfs-quota and gpfs-group-quota commands\n</code></pre> <ul> <li><code>Home</code> is <code>/gpfs/home/$USER</code> and <code>/home/$USER/</code></li> <li><code>Work</code> is <code>/gpfs/work/$USER/</code> and <code>/work/$USER</code></li> </ul> <p>To get a more detailed report about your user-level quotas, run <code>gpfs-quota</code>.</p> <pre><code>Filesystem Fileset    type         blocks      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks\nscale      share      USR               0          0          0          0     none |        1       0        0        0     none trixie3500-scale.gpfs.net\nscale      home       USR           20.7G        50G        51G     426.2M     none |   369754 1000000  1100000       60     none trixie3500-scale.gpfs.net\nscale      projects   USR          59.35T          0          0     1.614G     none |  9892887       0        0      419     none trixie3500-scale.gpfs.net\nscale      work       USR          112.2G       500G       550G          0     none |   167220 1000000  1100000        0     none trixie3500-scale.gpfs.net\n</code></pre> <p>To get a more detailed report about your group level quotas run <code>gpfs-group-quota</code>.</p> <pre><code>Disk quotas for group ai4d-cluster-aero-apdc-01-project (gid 171971877):\n                         Block Limits                                               |     File Limits\nFilesystem Fileset    type         blocks      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks\nscale      projects   GRP               0       100G       100G          0     none |        1       0        0        0     none trixie3500-scale.gpfs.net\n\nDisk quotas for group ai4d-cluster-ai4d-core-03-project (gid 171971331):\n                         Block Limits                                               |     File Limits\nFilesystem Fileset    type         blocks      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks\nscale      projects   GRP               0       100G       100G          0     none |        1       0        0        0     none trixie3500-scale.gpfs.net\n\nDisk quotas for group ai4d-cluster-ai4d-core-08-project (gid 171971449):\n                         Block Limits                                               |     File Limits\nFilesystem Fileset    type         blocks      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks\nscale      projects   GRP          2.339T     2.671T     2.671T          0     none |   130669       0        0        0     none trixie3500-scale.gpfs.net\n\n...\n\nDisk quotas for group ai4d-cluster-covid-covid-02-project (gid 171971328):\n                         Block Limits                                               |     File Limits\nFilesystem Fileset    type         blocks      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks\nscale      projects   GRP               0       100G       100G          0     none |        1       0        0        0     none trixie3500-scale.gpfs.net\n</code></pre>"},{"location":"quota/#requesting-additional-space-for-projects","title":"Requesting additional space for <code>/projects</code>","text":"<p>Please reach out to Research Platform Support rps-spr@nrc-cnrc.gc.ca to request additional space for projects. Existing quotas were based on the greater of either 100GiB or (current usage + current-usage/total-project-usage * 20TiB).</p>"},{"location":"quota/#cleaning-to-free-up-some-space","title":"Cleaning to Free Up Some Space","text":"<p>Here are some tools and hints to help you reduce space and inodes.</p>"},{"location":"quota/#using-gdu","title":"Using <code>gdu</code>","text":"<p>gdu is a fast disk usage analyzer with console interface written in Go.</p> <pre><code>gdu ~ Use arrow keys to navigate, press ? for help\n--- /gpfs/projects/DT/mtp/pkgs ---\n  32.8 GiB \u2588\u2588\u2588\u2588\u2588\u2588\u258d   \u258f/miniforge3\n  13.5 GiB \u2588\u2588\u258c       \u258f/miniconda3.disabled\n   4.7 GiB \u2589         \u258f/ollama-0.9.0\n  86.0 MiB           \u258fMiniforge3-Linux-x86_64.sh\n  32.2 MiB           \u258f/uv\n  17.8 MiB           \u258f/mitlm-0.4.2\n  13.7 MiB           \u258f/kenlm-2024-05-22\n\n Total disk usage: 51.1 GiB Apparent size: 50.8 GiB Items: 449164 Sorting by: size desc\n</code></pre> <p><code>gdu</code> allows you to delete files or directories by pressing <code>d</code>. If you are interested to find where you are losing your inodes, you can press <code>C</code> which <code>Sort by file count (asc/desc)</code>.</p>"},{"location":"quota/#diskonaut","title":"<code>diskonaut</code>","text":"<p>go DiskUsage(gdu) is a terminal disk space navigator \ud83d\udd2d.</p> <pre><code> Total: 55.7G (449164 files), freed: 0 | /gpfs/projects/DT/mtp/pkgs\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u2502   miniconda3.disabled/    \u2502\n\u2502                                                \u2502                           \u2502\n\u2502       miniforge3/ (+307991 descendants)        \u2502                           \u2502\n\u2502                                                \u2502        14.0G (25%)        \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                  36.9G (66%)                   \u2502                           \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2524\n\u2502                                                \u2502   ollama-0.9.0/ (+30)    \u2502\u2502\n\u2502                                                \u2502                          \u2502\u2502\n\u2502                                                \u2502        4.7G (8%)         \u2502\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2518\n(x = Small files)\n \u2190\u2193\u2191\u2192/&lt;ENTER&gt;/&lt;ESC&gt;: navigate, &lt;BACKSPACE&gt;: del\n</code></pre>"},{"location":"quota/#dust","title":"<code>dust</code>","text":"<p>dust is a more intuitive version of du in rust.</p> <pre><code>dust\n4.8G             \u250c\u2500\u2500 site-packages\u2502\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2592\u2592\u2592                               \u2502   9%\n4.9G           \u250c\u2500\u2534 python3.12     \u2502\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2592\u2592\u2592                               \u2502  10%\n4.9G         \u250c\u2500\u2534 lib              \u2502\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2592\u2592\u2592                               \u2502  10%\n5.0G       \u250c\u2500\u2534 comet-2.2.2        \u2502\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2592\u2592\u2592                               \u2502  10%\n5.7G       \u2502     \u250c\u2500\u2500 site-packages\u2502\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2592\u2592                               \u2502  11%\n5.7G       \u2502   \u250c\u2500\u2534 python3.12     \u2502\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2592\u2592                               \u2502  11%\n5.7G       \u2502 \u250c\u2500\u2534 lib              \u2502\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2592\u2592                               \u2502  11%\n5.8G       \u251c\u2500\u2534 transformers-4.44.2\u2502\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2592\u2592\u2592                               \u2502  11%\n 12G     \u250c\u2500\u2534 envs                 \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                               \u2502  25%\n 13G   \u250c\u2500\u2534 miniconda3.disabled    \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                               \u2502  26%\n5.5G   \u2502     \u250c\u2500\u2500 site-packages    \u2502\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591               \u2502  11%\n5.5G   \u2502   \u250c\u2500\u2534 python3.12         \u2502\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591               \u2502  11%\n5.6G   \u2502 \u250c\u2500\u2534 lib                  \u2502\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591               \u2502  11%\n5.4G   \u2502 \u2502       \u250c\u2500\u2500 site-packages\u2502\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591               \u2502  11%\n5.4G   \u2502 \u2502     \u250c\u2500\u2534 python3.12     \u2502\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591               \u2502  11%\n5.5G   \u2502 \u2502   \u250c\u2500\u2534 lib              \u2502\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591               \u2502  11%\n5.5G   \u2502 \u2502 \u250c\u2500\u2534 comet-2.2.2        \u2502\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591               \u2502  11%\n7.6G   \u2502 \u251c\u2500\u2534 envs                 \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591               \u2502  15$\n 19G   \u2502 \u251c\u2500\u2500 pkgs                 \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591               \u2502  38%\n 32G   \u251c\u2500\u2534 miniforge3             \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               \u2502  64%\n 51G \u250c\u2500\u2534 .                        \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502 100%\n</code></pre>"},{"location":"quota/#uv","title":"<code>uv</code>","text":"<p><code>uv</code> is a python virtual environment manager. It caches a lot of its downloaded libraries and this can take a lot of space and inodes. To reclaim some space and inodes you can do</p> <pre><code>uv cache clean\n</code></pre>"},{"location":"quota/#ncdu","title":"<code>ncdu</code>","text":"<p>ncdu is a nCurses terminal-user-interface which will walk the specified directory tree then provide a user interface to navigate it and display disk and inode usage.</p> <pre><code>ncdu 1.19 ~ Use the arrow keys to navigate, press ? for help\n--- /home/lover/git/trixie/trixie/docs -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  692.0 KiB [##################################] /files\n  280.0 KiB [#############                     ] /images\n   44.0 KiB [##                                ]  BeAGoodClusterCitizen.html\n   20.0 KiB [                                  ]  Trixie-Status.md\n   16.0 KiB [                                  ]  File-Transfers.md\n   12.0 KiB [                                  ]  quota.md\n   12.0 KiB [                                  ]  Internal-Access-Setup.md\n   12.0 KiB [                                  ]  jobs-jupyterlab.md\n   12.0 KiB [                                  ]  External-Access-Setup.md\n    8.0 KiB [                                  ]  jobs-abinit.md\n    8.0 KiB [                                  ]  index.md\n    8.0 KiB [                                  ]  jobs-conda-RAPIDS.md\n    8.0 KiB [                                  ]  Account-Codes.md\n    8.0 KiB [                                  ]  SLURM,-pytorch-distributed-and-Multiple-Nodes.md\n    4.0 KiB [                                  ]  Internal-Access-Advanced-Configuration.md\n    4.0 KiB [                                  ]  External-Access-Advanced-Configuration.md\n    4.0 KiB [                                  ]  jobs-python-virtualenv.md\n    4.0 KiB [                                  ]  Networking-and-connectivity.md\n    4.0 KiB [                                  ]  Automatically-Resuming-Requeueing.md\n    4.0 KiB [                                  ]  Jobs-conda-pytorch.md\n    4.0 KiB [                                  ]  Hardware.md\n    4.0 KiB [                                  ]  Available-Software.md\n    4.0 KiB [                                  ]  External-HPC-Systems.md\n*Total disk usage:   1.2 MiB   Apparent size:   1.1 MiB   Items: 53\n</code></pre>"}]}